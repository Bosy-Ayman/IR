{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bosy-Ayman/IR/blob/main/Week(13)_IR_Transformers_%26_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZzUxXUVMVVU"
      },
      "source": [
        "\n",
        "\n",
        "# **DSAI 201 - Information Retrieval - Zewail City**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kjTczIiMctt"
      },
      "source": [
        "This lab aims to build basic practical knowledge on how to use BERT transformer model.\n",
        "\n",
        "The **learning outcomes** of the this notebook are:\n",
        "1. Understand what is huggingface and how to use it.\n",
        "2. Learn how to tokenize a sentence and convert an input sentence to the required format for BERT (deal with special tokens, sentence length & Attention Mask)\n",
        "3. Perform tokenization on a given dataset.\n",
        "4. Check the architecture of each layer in BERT in practice.\n",
        "5. Get to know the output of BERT\n",
        "6. Utilize BERT embedding in computing cosine similarity.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1vSBU5h9U_C"
      },
      "source": [
        "## Utilize the GPU of Colab\n",
        "In this session, we will work on experiments that require GPU to run. To make the experiments running over the GPU provided by Colab, you need to do the following:\n",
        "\n",
        "1. Go to Menu > Runtime > Change runtime.\n",
        "\n",
        "2. Change hardware acceleration to GPU.\n",
        "\n",
        "Then run the following cell to confirm that the GPU is detected."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hITeSSYGMqPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0I7C40zV12G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8eded3d-668b-427e-f3dc-1613dba3806f"
      },
      "source": [
        "import torch\n",
        "\n",
        "# Choose GPU as device to run the experiments on\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkS6LLkX6HHV"
      },
      "source": [
        "## **Hugging Face**\n",
        "[Hugging face](https://huggingface.co/) is an NLP-focused startup with a large open-source community, in particular around the Transformers library. ðŸ¤— Transformers is a python-based library that exposes an API to use many well-known transformer architectures, such as BERT, RoBERTa, GPT-2 or DistilBERT, that obtain state-of-the-art results on a variety of NLP tasks like text classification, information extraction, question answering, and text generation. Those architectures come pre-trained with several sets of weights. Getting started with Transformers only requires to install the pip package:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6sKgPMd_-gU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e4371a9-c6e0-4e38-8160-ac7d6b57b40d"
      },
      "source": [
        "#install the transformer library\n",
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmC0oRLO_2c6"
      },
      "source": [
        "#we need to import the following libraries.\n",
        "import pandas as pd\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "\n",
        "#to display the full text on the notebook without truncation\n",
        "pd.set_option('display.max_colwidth', 150)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIzygfXzAILT"
      },
      "source": [
        "### Transformer Components in Hugging face\n",
        "Transformers is based around the concept of pre-trained transformer models. These transformer models come in different shapes, sizes, and architectures and have their own ways of accepting input data: via tokenization.\n",
        "\n",
        "The library builds on three main classes:\n",
        "1. **The configuration class:** hosts relevant information concerning the model we will be using, such as the number of layers and the number of attention heads. Below is an example of a BERT configuration file, for the pre-trained weights bert-base-cased. The configuration classes host these attributes with various I/O methods and standardized name properties.\n",
        "\n",
        "{\n",
        "  \"attention_probs_dropout_prob\": 0.1,\n",
        "  \"hidden_act\": \"gelu\",\n",
        "  \"hidden_dropout_prob\": 0.1,\n",
        "  \"hidden_size\": 768,\n",
        "  \"initializer_range\": 0.02,\n",
        "  \"intermediate_size\": 3072,\n",
        "  \"max_position_embeddings\": 512,\n",
        "  \"num_attention_heads\": 12,\n",
        "  \"num_hidden_layers\": 12,\n",
        "  \"type_vocab_size\": 2,\n",
        "  \"vocab_size\": 28996\n",
        "}\n",
        "\n",
        "2. **The tokenizer class:** the tokenizer class takes care of converting python string in arrays or tensors of integers which are indices in a model vocabulary. It has many handy features revolving around the tokenization of a string into tokens. This tokenization varies according to the model, therefore each model has its own tokenizer.\n",
        "3. **The model class:** the model class holds the neural network modeling logic itself."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfT-xpdo8WBv"
      },
      "source": [
        "## Loading BERT model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riTqmkPD8cib"
      },
      "source": [
        "In the following example, we show how to load BERT model. BERT is one of the famous models in literature and performs well for many language tasks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adAChFH6AN1C"
      },
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "model_name = \"bert-base-uncased\"\n",
        "\n",
        "bert_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "bert_model = AutoModel.from_pretrained(model_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwvYVmK5WOD-"
      },
      "source": [
        "## **Perform tokenization**\n",
        "\n",
        "To feed our text to BERT, it must be splitted into tokens, and then these tokens must be mapped to their index in the tokenizer vocabulary.\n",
        "\n",
        "The tokenization must be performed by the tokenizer included with BERT tokenizer. Now, we utilize the pretrained BERT model to tokenize a given sentence. We just need to provide the sentence as an input string to the loaded tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsIEdqNl_zDO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bb446ca-c808-49b5-b2ab-9a834b2362f3"
      },
      "source": [
        "## Example 1 of tokenization\n",
        "text1 = \"This is week 8 of the Information Retrieval course. Today's lesson is very important in the field of natural language processing.\"\n",
        "tokeninzed_text1 = bert_tokenizer.tokenize(text1)\n",
        "text1_token_ids = bert_tokenizer.convert_tokens_to_ids(tokeninzed_text1)\n",
        "\n",
        "# Print the original sentence.\n",
        "print('Original text1: ', text1)\n",
        "\n",
        "# Print the sentence split into tokens.\n",
        "print('Tokenized text1 : ', tokeninzed_text1)\n",
        "\n",
        "# Print the sentence mapped to token ids.\n",
        "print('Token IDs of text1: ',text1_token_ids )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original text1:  This is week 8 of the Information Retrieval course. Today's lesson is very important in the field of natural language processing.\n",
            "Tokenized text1 :  ['this', 'is', 'week', '8', 'of', 'the', 'information', 'retrieval', 'course', '.', 'today', \"'\", 's', 'lesson', 'is', 'very', 'important', 'in', 'the', 'field', 'of', 'natural', 'language', 'processing', '.']\n",
            "Token IDs of text1:  [2023, 2003, 2733, 1022, 1997, 1996, 2592, 26384, 2607, 1012, 2651, 1005, 1055, 10800, 2003, 2200, 2590, 1999, 1996, 2492, 1997, 3019, 2653, 6364, 1012]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkECQs4BXl3N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42b26a4c-d8f8-455e-dbd8-e8149c218029"
      },
      "source": [
        "## Example 1 of tokenization\n",
        "text1 = \"This is week 8 of the Information Retrieval course. Today's lesson is very important in the field of natural language processing.\"\n",
        "tokeninzed_text1 = bert_tokenizer.tokenize(text1, add_special_tokens=True)\n",
        "text1_token_ids = bert_tokenizer.convert_tokens_to_ids(tokeninzed_text1)\n",
        "\n",
        "# Print the original sentence.\n",
        "print('Original text1: ', text1)\n",
        "\n",
        "# Print the sentence split into tokens.\n",
        "print('Tokenized text1 : ', tokeninzed_text1)\n",
        "\n",
        "# Print the sentence mapped to token ids.\n",
        "print('Token IDs of text1: ',text1_token_ids )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original text1:  This is week 8 of the Information Retrieval course. Today's lesson is very important in the field of natural language processing.\n",
            "Tokenized text1 :  ['[CLS]', 'this', 'is', 'week', '8', 'of', 'the', 'information', 'retrieval', 'course', '.', 'today', \"'\", 's', 'lesson', 'is', 'very', 'important', 'in', 'the', 'field', 'of', 'natural', 'language', 'processing', '.', '[SEP]']\n",
            "Token IDs of text1:  [101, 2023, 2003, 2733, 1022, 1997, 1996, 2592, 26384, 2607, 1012, 2651, 1005, 1055, 10800, 2003, 2200, 2590, 1999, 1996, 2492, 1997, 3019, 2653, 6364, 1012, 102]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyKXDJf0p_Lp"
      },
      "source": [
        "## **BERT Required Formatting**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mB-qA3T9qoBs"
      },
      "source": [
        "The input for BERT model has to be in specific format. What we need is to:\n",
        "1. Add special tokens to the start and end of each sentence.\n",
        "2. Pad & truncate all sentences to a single constant length. Maximum allowed length is 512.\n",
        "3. Explicitly differentiate real tokens from padding tokens with the \"attention mask\".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qv-dYLJEaqcl"
      },
      "source": [
        "<!-- **`[SEP]`** -->\n",
        "\n",
        "At the end of every sentence, we need to append the special **`[SEP]`** token.\n",
        "\n",
        "This token is an artifact of two-sentence tasks, where BERT is given two separate sentences and asked to do some task on them (e.g., can the answer to the question in sentence A be found in sentence B?)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "He4MI90SbJHg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f558a95e-dffe-4e4d-c691-83ce333b53cd"
      },
      "source": [
        "sep_token =bert_tokenizer.sep_token\n",
        "\n",
        "# print sep token of the tokenizer\n",
        "print(\"Sep token : \", sep_token)\n",
        "\n",
        "# print the token id of sep token\n",
        "print('Token ID of sep token : ',  bert_tokenizer.convert_tokens_to_ids(sep_token))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sep token :  [SEP]\n",
            "Token ID of sep token :  102\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeI0dOoPbD_a"
      },
      "source": [
        "<!-- **`[CLS]`** -->\n",
        "\n",
        ">  \"The first token of every sequence is always a special classification token (**`[CLS]`**). The final hidden state\n",
        "corresponding to this token is used as the aggregate sequence representation for classification\n",
        "tasks.\" ([BERT paper](https://arxiv.org/pdf/1810.04805.pdf))\n",
        "\n",
        "\n",
        "This token has special significance. BERT consists of 12 Transformer layers. Each transformer takes in a list of token embeddings, and produces the same number of embeddings on the output.\n",
        "\n",
        "On the output of the final (12th) transformer, *only the first embedding (corresponding to the [CLS] token) is used by the classifier.\n",
        "\n",
        "![Illustration of CLS token purpose](https://drive.google.com/uc?export=view&id=1ck4mvGkznVJfW3hv6GUqcdGepVTOx7HE)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceLmvDiTq6oW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "717d59ee-60cd-4cd8-ea41-d377da8cca01"
      },
      "source": [
        "cls_token =bert_tokenizer.cls_token\n",
        "\n",
        "# print cls token of the tokenizer\n",
        "print(\"Cls token : \", cls_token)\n",
        "\n",
        "# print the token id of cls token\n",
        "print('Token ID of cls token : ',  bert_tokenizer.convert_tokens_to_ids(cls_token))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cls token :  [CLS]\n",
            "Token ID of cls token :  101\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_dkV6D4rgOI"
      },
      "source": [
        "### **Sentence Length & Attention Mask**\n",
        "\n",
        "BERT has two constraints:\n",
        "1. All sentences must be padded or truncated to a single, fixed length.\n",
        "2. The maximum sentence length is 512 tokens.\n",
        "\n",
        "Padding is done with a special **`[PAD]`** token, which is at index 0 in the BERT vocabulary.\n",
        "\n",
        "The **\"Attention Mask\"** is simply an array of 1s and 0s indicating which tokens are padding and which aren't. This mask tells the \"Self-Attention\" mechanism in BERT not to incorporate these PAD tokens into its interpretation of the sentence.\n",
        "\n",
        "The below illustration demonstrates padding out to a \"MAX_LEN\" of 8 tokens.\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1cb5xeqLu_5vPOgs3eRnail2Y00Fl2pCo\" width=\"600\">\n",
        "\n",
        "\n",
        "\n",
        "The maximum length does impact training and evaluation speed.\n",
        "For example, with a Tesla K80:\n",
        "\n",
        "`MAX_LEN = 128  -->  Training epochs take ~5:28 each`\n",
        "\n",
        "`MAX_LEN = 64   -->  Training epochs take ~2:57 each`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSF2lO71gf_L"
      },
      "source": [
        "### Perform tokenization on a sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnQz-uyzrsRd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd761931-3e87-4beb-84ff-7c2e3b49c943"
      },
      "source": [
        "# `encode_plus` will:\n",
        "#   (1) Tokenize the sentence.\n",
        "#   (2) Prepend the `[CLS]` token to the start.\n",
        "#   (3) Append the `[SEP]` token to the end.\n",
        "#   (4) Map tokens to their IDs.\n",
        "#   (5) Pad or truncate the sentence to `max_length`\n",
        "#   (6) Create attention masks for [PAD] tokens.\n",
        "\n",
        "text = \"Today we will learn how to use the BERT model in practice and conduct some experiments.\"\n",
        "encoding= bert_tokenizer.encode_plus(\n",
        "                  text,                      # Sentence to encode.\n",
        "                  add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                  truncation=True,\n",
        "                  max_length = 32,           # Pad & truncate all sentences.\n",
        "                  padding=\"max_length\",\n",
        "                  return_attention_mask = True,   # Construct attention mask\n",
        "                  return_tensors = 'pt',     # Return pytorch tensors.\n",
        "              )\n",
        "\n",
        "\n",
        "# Print the input ids and attention mask of the encoded sentence\n",
        "print(\"Original text: \", text)\n",
        "print(\"Input ids: \", encoding[\"input_ids\"].flatten(),)\n",
        "print(\"Attention mask: \", encoding[\"attention_mask\"].flatten(),)\n",
        "# Note in the output of the next line that the cls, sep,and pad tokens were added automatically\n",
        "print(\"Tokenized text: \",bert_tokenizer.convert_ids_to_tokens(encoding[\"input_ids\"].flatten()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original text:  Today we will learn how to use the BERT model in practice and conduct some experiments.\n",
            "Input ids:  tensor([  101,  2651,  2057,  2097,  4553,  2129,  2000,  2224,  1996, 14324,\n",
            "         2944,  1999,  3218,  1998,  6204,  2070,  7885,  1012,   102,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0])\n",
            "Attention mask:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0])\n",
            "Tokenized text:  ['[CLS]', 'today', 'we', 'will', 'learn', 'how', 'to', 'use', 'the', 'bert', 'model', 'in', 'practice', 'and', 'conduct', 'some', 'experiments', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZY1yWSgr_jj"
      },
      "source": [
        "## **Dataset tokenization**\n",
        "\n",
        "Let's tokenize the EveTAR dataset.\n",
        "First, we need to load the dataset.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzjCZeSSsRh2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 727
        },
        "outputId": "30cd7fd5-5e54-49c0-833e-4949d73f2939"
      },
      "source": [
        "imdb_dataset_url = \"https://raw.githubusercontent.com/LearnDataSci/articles/master/Python%20Pandas%20Tutorial%20A%20Complete%20Introduction%20for%20Beginners/IMDB-Movie-Data.csv\"\n",
        "\n",
        "reviews = pd.read_csv(imdb_dataset_url)\n",
        "reviews.head()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Rank                    Title                     Genre  \\\n",
              "0     1  Guardians of the Galaxy   Action,Adventure,Sci-Fi   \n",
              "1     2               Prometheus  Adventure,Mystery,Sci-Fi   \n",
              "2     3                    Split           Horror,Thriller   \n",
              "3     4                     Sing   Animation,Comedy,Family   \n",
              "4     5            Suicide Squad  Action,Adventure,Fantasy   \n",
              "\n",
              "                                                                                                                                             Description  \\\n",
              "0                        A group of intergalactic criminals are forced to work together to stop a fanatical warrior from taking control of the universe.   \n",
              "1                        Following clues to the origin of mankind, a team finds a structure on a distant moon, but they soon realize they are not alone.   \n",
              "2  Three girls are kidnapped by a man with a diagnosed 23 distinct personalities. They must try to escape before the apparent emergence of a frightfu...   \n",
              "3  In a city of humanoid animals, a hustling theater impresario's attempt to save his theater with a singing competition becomes grander than he anti...   \n",
              "4  A secret government agency recruits some of the most dangerous incarcerated super-villains to form a defensive task force. Their first mission: sa...   \n",
              "\n",
              "               Director  \\\n",
              "0            James Gunn   \n",
              "1          Ridley Scott   \n",
              "2    M. Night Shyamalan   \n",
              "3  Christophe Lourdelet   \n",
              "4            David Ayer   \n",
              "\n",
              "                                                                       Actors  \\\n",
              "0                        Chris Pratt, Vin Diesel, Bradley Cooper, Zoe Saldana   \n",
              "1     Noomi Rapace, Logan Marshall-Green, Michael Fassbender, Charlize Theron   \n",
              "2            James McAvoy, Anya Taylor-Joy, Haley Lu Richardson, Jessica Sula   \n",
              "3  Matthew McConaughey,Reese Witherspoon, Seth MacFarlane, Scarlett Johansson   \n",
              "4                          Will Smith, Jared Leto, Margot Robbie, Viola Davis   \n",
              "\n",
              "   Year  Runtime (Minutes)  Rating   Votes  Revenue (Millions)  Metascore  \n",
              "0  2014                121     8.1  757074              333.13       76.0  \n",
              "1  2012                124     7.0  485820              126.46       65.0  \n",
              "2  2016                117     7.3  157606              138.12       62.0  \n",
              "3  2016                108     7.2   60545              270.32       59.0  \n",
              "4  2016                123     6.2  393727              325.02       40.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-42779730-9aa9-4c74-b257-044908f23eb0\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Rank</th>\n",
              "      <th>Title</th>\n",
              "      <th>Genre</th>\n",
              "      <th>Description</th>\n",
              "      <th>Director</th>\n",
              "      <th>Actors</th>\n",
              "      <th>Year</th>\n",
              "      <th>Runtime (Minutes)</th>\n",
              "      <th>Rating</th>\n",
              "      <th>Votes</th>\n",
              "      <th>Revenue (Millions)</th>\n",
              "      <th>Metascore</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Guardians of the Galaxy</td>\n",
              "      <td>Action,Adventure,Sci-Fi</td>\n",
              "      <td>A group of intergalactic criminals are forced to work together to stop a fanatical warrior from taking control of the universe.</td>\n",
              "      <td>James Gunn</td>\n",
              "      <td>Chris Pratt, Vin Diesel, Bradley Cooper, Zoe Saldana</td>\n",
              "      <td>2014</td>\n",
              "      <td>121</td>\n",
              "      <td>8.1</td>\n",
              "      <td>757074</td>\n",
              "      <td>333.13</td>\n",
              "      <td>76.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Prometheus</td>\n",
              "      <td>Adventure,Mystery,Sci-Fi</td>\n",
              "      <td>Following clues to the origin of mankind, a team finds a structure on a distant moon, but they soon realize they are not alone.</td>\n",
              "      <td>Ridley Scott</td>\n",
              "      <td>Noomi Rapace, Logan Marshall-Green, Michael Fassbender, Charlize Theron</td>\n",
              "      <td>2012</td>\n",
              "      <td>124</td>\n",
              "      <td>7.0</td>\n",
              "      <td>485820</td>\n",
              "      <td>126.46</td>\n",
              "      <td>65.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Split</td>\n",
              "      <td>Horror,Thriller</td>\n",
              "      <td>Three girls are kidnapped by a man with a diagnosed 23 distinct personalities. They must try to escape before the apparent emergence of a frightfu...</td>\n",
              "      <td>M. Night Shyamalan</td>\n",
              "      <td>James McAvoy, Anya Taylor-Joy, Haley Lu Richardson, Jessica Sula</td>\n",
              "      <td>2016</td>\n",
              "      <td>117</td>\n",
              "      <td>7.3</td>\n",
              "      <td>157606</td>\n",
              "      <td>138.12</td>\n",
              "      <td>62.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Sing</td>\n",
              "      <td>Animation,Comedy,Family</td>\n",
              "      <td>In a city of humanoid animals, a hustling theater impresario's attempt to save his theater with a singing competition becomes grander than he anti...</td>\n",
              "      <td>Christophe Lourdelet</td>\n",
              "      <td>Matthew McConaughey,Reese Witherspoon, Seth MacFarlane, Scarlett Johansson</td>\n",
              "      <td>2016</td>\n",
              "      <td>108</td>\n",
              "      <td>7.2</td>\n",
              "      <td>60545</td>\n",
              "      <td>270.32</td>\n",
              "      <td>59.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Suicide Squad</td>\n",
              "      <td>Action,Adventure,Fantasy</td>\n",
              "      <td>A secret government agency recruits some of the most dangerous incarcerated super-villains to form a defensive task force. Their first mission: sa...</td>\n",
              "      <td>David Ayer</td>\n",
              "      <td>Will Smith, Jared Leto, Margot Robbie, Viola Davis</td>\n",
              "      <td>2016</td>\n",
              "      <td>123</td>\n",
              "      <td>6.2</td>\n",
              "      <td>393727</td>\n",
              "      <td>325.02</td>\n",
              "      <td>40.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-42779730-9aa9-4c74-b257-044908f23eb0')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-42779730-9aa9-4c74-b257-044908f23eb0 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-42779730-9aa9-4c74-b257-044908f23eb0');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-6a9e3e9a-f605-4114-a41c-e5f5b98c5e03\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6a9e3e9a-f605-4114-a41c-e5f5b98c5e03')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-6a9e3e9a-f605-4114-a41c-e5f5b98c5e03 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "reviews",
              "summary": "{\n  \"name\": \"reviews\",\n  \"rows\": 1000,\n  \"fields\": [\n    {\n      \"column\": \"Rank\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 288,\n        \"min\": 1,\n        \"max\": 1000,\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          522,\n          738,\n          741\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 999,\n        \"samples\": [\n          \"Olympus Has Fallen\",\n          \"Man on a Ledge\",\n          \"The Girl with All the Gifts\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Genre\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 207,\n        \"samples\": [\n          \"Mystery,Romance,Sci-Fi\",\n          \"Drama,Mystery,Sci-Fi\",\n          \"Drama,Mystery,Romance\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Description\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          \"A lawyer finds himself in over his head when he gets involved in drug trafficking.\",\n          \"A CIA agent on the ground in Jordan hunts down a powerful terrorist leader while being caught between the unclear intentions of his American supervisors and Jordan Intelligence.\",\n          \"A titan of industry is sent to prison after she's caught insider trading. When she emerges ready to rebrand herself as America's latest sweetheart, not everyone she screwed over is so quick to forgive and forget.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Director\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 644,\n        \"samples\": [\n          \"Patricia Riggen\",\n          \"Gregory Wilson\",\n          \"Chris McCoy\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Actors\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 996,\n        \"samples\": [\n          \"Adrian Titieni, Maria-Victoria Dragus, Lia Bugnar,Malina Manovici\",\n          \"Madina Nalwanga, David Oyelowo, Lupita Nyong'o, Martin Kabanza\",\n          \"Ry\\u00fbnosuke Kamiki, Mone Kamishiraishi, Ry\\u00f4 Narita, Aoi Yuki\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Year\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 2006,\n        \"max\": 2016,\n        \"num_unique_values\": 11,\n        \"samples\": [\n          2011,\n          2014,\n          2010\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Runtime (Minutes)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 18,\n        \"min\": 66,\n        \"max\": 191,\n        \"num_unique_values\": 94,\n        \"samples\": [\n          106,\n          99,\n          146\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Rating\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.9454287892779634,\n        \"min\": 1.9,\n        \"max\": 9.0,\n        \"num_unique_values\": 55,\n        \"samples\": [\n          7.4,\n          6.1,\n          4.1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Votes\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 188762,\n        \"min\": 61,\n        \"max\": 1791916,\n        \"num_unique_values\": 997,\n        \"samples\": [\n          214994,\n          4370,\n          23713\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Revenue (Millions)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 103.25354047492472,\n        \"min\": 0.0,\n        \"max\": 936.63,\n        \"num_unique_values\": 814,\n        \"samples\": [\n          89.02,\n          23.23,\n          202.85\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Metascore\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 17.194757023263833,\n        \"min\": 11.0,\n        \"max\": 100.0,\n        \"num_unique_values\": 84,\n        \"samples\": [\n          27.0,\n          76.0,\n          47.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0IMRifzoCm_"
      },
      "source": [
        "Now, we need to encode each tweet in this dataset.\n",
        "Encoding the dataset is your job now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LF1Uc3VroMy4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07c1bddf-774c-40ed-d48a-3d0e9f26b8b3"
      },
      "source": [
        "def encode(text, max_length=32):\n",
        "    return bert_tokenizer.encode_plus(\n",
        "                  text,                      # Sentence to encode.\n",
        "                  add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                  truncation=True,\n",
        "                  max_length = max_length,           # Pad & truncate all sentences.\n",
        "                  padding=\"max_length\",\n",
        "                  return_attention_mask = True,   # Construct attention mask\n",
        "                  return_tensors = 'pt',     # Return pytorch tensors.\n",
        "    )\n",
        "\n",
        "tokenized_reviews = []\n",
        "for tweet in tqdm(reviews[\"Description\"].values, desc=\"Tokenizing ...\"):\n",
        "    tokenized_reviews.append(encode(tweet, max_length=32))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Tokenizing ...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 2265.40it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KN3bFuTZpjLR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5a608d2-2572-4ecc-a328-e7f88e7e162c"
      },
      "source": [
        "tokenized_reviews[0]\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[  101,  1037,  2177,  1997,  6970,  9692, 28804, 12290,  2024,  3140,\n",
              "          2000,  2147,  2362,  2000,  2644,  1037,  5470, 12070,  2389,  6750,\n",
              "          2013,  2635,  2491,  1997,  1996,  5304,  1012,   102,     0,     0,\n",
              "             0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 0, 0, 0, 0]])}"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2mlvWlZhIt6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecd650a9-3377-42cb-aca3-4fe5fcc196c5"
      },
      "source": [
        "len(tokenized_reviews)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3GGDjXvuyrw"
      },
      "source": [
        "## BERT layers\n",
        "\n",
        "Let's see in practice the layers that consist the BERT model. Simply, you can see every layer in the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hn8_4baOpqmu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0121ab37-ba2d-4268-abcf-b62845aa758f"
      },
      "source": [
        "bert_model.cuda()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertModel(\n",
              "  (embeddings): BertEmbeddings(\n",
              "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (token_type_embeddings): Embedding(2, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): BertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0-11): 12 x BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): BertPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KA-HvUt_Yedy"
      },
      "source": [
        "## BERT output\n",
        "\n",
        "Let's see what is the output of BERT for a given sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOwBC5Rwt8jk"
      },
      "source": [
        "input_ids = tokenized_reviews[0][\"input_ids\"].to(device)\n",
        "attention_mask = tokenized_reviews[0][\"attention_mask\"].to(device)\n",
        "output = bert_model(input_ids=input_ids, attention_mask=attention_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOWnMehpv69i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1eea81c1-8b5f-4c1b-90a6-8e005c4ed20c"
      },
      "source": [
        "output[0].shape # batch_size x sequence_length x embedding_dimension"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 32, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16v0JoHSw74f"
      },
      "source": [
        " Let's see the embedding vector for the tokens. In this case, we have 32 tokens. Each token has embedding vector of length 768"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHXVqh34wWPn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95d9be25-425f-413d-b8a4-38111d214b7d"
      },
      "source": [
        "# print the embedding of all input tokens.\n",
        "all_embeddings = output[0][0]\n",
        "print(all_embeddings.shape)\n",
        "print(all_embeddings)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 768])\n",
            "tensor([[-0.5374, -0.0045, -0.1489,  ..., -0.1589,  0.2519,  0.3107],\n",
            "        [-0.7558,  0.0240, -0.7778,  ..., -0.0435,  0.4008, -0.0553],\n",
            "        [ 0.1333,  0.0783, -0.0168,  ..., -0.4033,  0.1526, -0.1878],\n",
            "        ...,\n",
            "        [ 0.0496, -0.2305,  0.0917,  ...,  0.0366,  0.1320, -0.0457],\n",
            "        [ 0.2256, -0.2471,  0.1439,  ...,  0.0176,  0.0063, -0.2181],\n",
            "        [ 0.1711, -0.2524,  0.0136,  ...,  0.0411,  0.0254, -0.1964]],\n",
            "       device='cuda:0', grad_fn=<SelectBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlGr-iRxxTb_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37b05524-e533-42a2-8e76-694b866f0463"
      },
      "source": [
        "# print the cls embedding\n",
        "cls_embedding = output[0][0][0]\n",
        "print(cls_embedding.shape)\n",
        "print(cls_embedding)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([768])\n",
            "tensor([-5.3743e-01, -4.5285e-03, -1.4889e-01,  2.6006e-01, -5.4038e-01,\n",
            "        -1.0482e-01,  5.7612e-02,  3.7673e-01,  2.9624e-01, -4.7522e-01,\n",
            "        -1.3860e-01, -1.2560e-01,  9.2107e-02,  1.1082e+00,  6.0340e-01,\n",
            "         2.4000e-01, -2.3380e-01,  1.6385e-01,  5.4327e-01, -1.8306e-01,\n",
            "         1.4615e-02, -6.7008e-01, -1.5706e-01,  4.8035e-01,  8.6288e-02,\n",
            "        -1.9906e-01, -3.1754e-01,  2.3249e-01,  4.4113e-01,  3.2665e-01,\n",
            "        -3.6641e-01,  4.9974e-02, -3.6367e-01, -7.1475e-01,  3.9699e-01,\n",
            "         2.8407e-01, -1.6459e-02,  2.3711e-01, -2.6904e-01,  3.8026e-01,\n",
            "        -4.8049e-01,  5.8538e-03,  2.1590e-01,  4.6976e-02, -8.9185e-02,\n",
            "        -8.1980e-01, -2.4482e+00, -3.5167e-01, -1.7509e-01,  4.3415e-01,\n",
            "         3.2940e-01, -2.5368e-01,  6.2703e-01, -1.2208e-01, -7.2259e-02,\n",
            "         6.8791e-01, -1.0008e-01,  4.4677e-01, -1.0553e-01,  4.7799e-01,\n",
            "         5.0411e-01,  3.2362e-01, -2.5777e-01, -1.8169e-01, -2.7259e-01,\n",
            "         1.5589e-01, -1.0433e-01,  1.7155e-01, -4.1691e-01,  3.6848e-01,\n",
            "        -9.1805e-01, -1.3195e-01, -9.7448e-02,  4.8465e-02, -2.2488e-01,\n",
            "         3.1816e-01, -3.3687e-01,  4.1722e-01, -4.6224e-01, -6.0765e-01,\n",
            "        -3.5041e-01,  8.5068e-01,  4.6530e-01,  5.4891e-01,  4.2097e-01,\n",
            "         4.4810e-01, -7.8100e-01, -3.3497e-01,  3.3150e-01,  2.1803e-01,\n",
            "        -2.9535e-01,  8.3976e-02, -1.6774e-01,  4.6643e-01,  1.4172e-01,\n",
            "        -1.1274e-02,  1.4190e-01, -9.6188e-02, -1.7654e-01,  6.1941e-01,\n",
            "         7.3568e-01,  1.8647e-01,  2.7864e-01, -6.8203e-01,  4.5745e-01,\n",
            "         2.4111e-01,  8.5509e-02,  3.2418e-02, -2.1655e-03, -2.2295e+00,\n",
            "         3.3141e-01, -1.1646e-01,  3.1883e-01, -5.2556e-01, -1.6126e-02,\n",
            "         1.1619e-01,  6.8892e-01, -1.1975e-01, -4.8647e-01, -5.4230e-01,\n",
            "         4.9899e-01,  6.9192e-02, -1.2856e-01,  2.5392e-01, -2.6832e-01,\n",
            "        -2.5112e-01, -9.4531e-02, -2.7654e-02,  2.8821e-01, -1.5544e-01,\n",
            "         4.1759e-01,  6.7428e-01, -2.4948e-01, -5.2835e-01,  1.3554e-01,\n",
            "         4.1580e-01,  1.7464e-01, -7.6731e-02, -4.3433e-01, -6.0331e-01,\n",
            "        -5.2344e-01, -5.5101e-01, -2.8958e+00,  5.3998e-02,  1.0366e+00,\n",
            "         6.4733e-01, -1.2813e-01, -2.8076e-01,  1.2702e-01, -2.7438e-01,\n",
            "         4.6522e-02,  1.0331e-01, -3.3084e-01,  9.5537e-02,  2.1632e-01,\n",
            "        -3.9647e-01,  3.4859e-02,  1.0773e-01,  5.4348e-01,  6.1812e-01,\n",
            "         8.0452e-01,  1.7791e-01, -2.5442e-01, -4.5418e-01,  1.7785e-02,\n",
            "         3.0386e-01,  9.1012e-01,  1.7543e-01, -3.1344e-01,  5.7207e-01,\n",
            "        -6.7174e-02,  4.5841e-01,  5.5699e-01, -1.6470e-01,  8.5065e-01,\n",
            "         4.1599e-02,  3.0489e-01,  6.6946e-01, -3.2103e-01, -2.0187e-01,\n",
            "        -4.7922e-01,  4.7672e-01,  4.1828e-01,  3.5868e-01,  2.2186e-01,\n",
            "        -1.5463e-01,  3.4956e-01, -4.3461e-01, -3.8787e-01,  4.1785e-01,\n",
            "        -3.8710e-01, -9.2497e-03,  1.1656e-01,  6.2701e-01,  5.5987e-01,\n",
            "        -1.7537e-01,  2.1895e-01, -4.1491e-01,  1.4390e-01,  1.9062e-01,\n",
            "        -2.6738e-01, -1.2868e-01, -5.1573e-01,  1.9854e-01, -1.1268e-01,\n",
            "         3.6034e+00,  3.1454e-01,  3.1609e-01,  5.3699e-01,  2.8585e-01,\n",
            "        -1.5063e-01, -2.0701e-01, -4.3576e-01,  1.0565e-01,  2.1226e-01,\n",
            "        -3.5915e-01,  1.9773e-02, -5.4193e-01, -3.9068e-01,  6.1542e-02,\n",
            "         3.4871e-01,  4.8209e-01,  9.6162e-02,  1.2915e-01, -1.2592e-01,\n",
            "         3.9506e-01,  7.7095e-02,  5.9230e-01, -1.2033e-01, -1.3435e+00,\n",
            "         3.6551e-01, -3.7919e-01, -3.2848e-01, -1.1940e-01, -5.4889e-01,\n",
            "        -4.1359e-01, -6.3698e-01, -3.5510e-01,  1.3602e-02, -8.6476e-02,\n",
            "        -5.7861e-01, -3.6100e-01,  6.7493e-01,  4.2027e-01,  1.5214e-01,\n",
            "         5.6726e-01,  1.1998e-01,  4.0017e-01,  3.7361e-02,  1.4489e-01,\n",
            "        -1.8854e-01,  1.6294e-01,  1.7520e-01, -7.9033e-02,  6.6366e-02,\n",
            "         7.6888e-02,  2.3771e-01,  2.6509e-01, -5.3019e-01, -4.2353e-02,\n",
            "         4.8594e-01, -3.4620e-01,  2.7110e-01,  1.1284e-01, -1.7680e-01,\n",
            "        -9.2132e-01, -6.8637e-02, -3.6995e-01,  9.7562e-03, -4.2284e-01,\n",
            "        -7.3610e-02, -2.3187e-01, -2.7064e-01, -3.4258e+00, -2.6256e-01,\n",
            "        -2.3200e-01,  2.0584e-01, -4.0772e-02,  1.1660e-01,  4.2480e-02,\n",
            "         6.9695e-01,  1.0058e-01, -1.4465e-01,  1.9573e-01,  7.2896e-02,\n",
            "         2.9824e-01, -1.1376e-01, -3.5106e-01,  1.9648e-01,  3.2997e-01,\n",
            "        -9.1438e-01, -6.5608e-01, -7.8920e-01, -9.6840e-02, -4.7325e-01,\n",
            "        -4.2105e-02,  2.6192e-01,  1.4847e-01,  5.0819e-01, -5.2159e-01,\n",
            "        -1.5826e-01, -1.0423e-01, -2.2784e-01,  2.6193e-02, -5.1557e-01,\n",
            "        -5.0252e-03,  6.7725e-02, -2.4176e-01, -1.3200e+00,  6.9681e-01,\n",
            "        -1.7353e-01, -1.0539e-01,  3.0057e-01,  2.3004e-01,  1.0123e+00,\n",
            "         9.1808e-02, -1.0048e+00,  2.9136e-01,  3.9483e-01,  2.2131e-01,\n",
            "         1.1378e+00,  3.5355e-01,  4.2432e-01, -4.0293e-01,  5.6315e-01,\n",
            "        -1.0339e-01, -2.2230e-02,  9.4146e-02,  4.0718e-01, -5.3985e-01,\n",
            "        -1.8037e-01, -8.1189e-01,  5.4671e-01,  1.7407e-01, -5.5176e-01,\n",
            "        -3.0513e-01, -1.3631e-01,  3.9558e-01,  4.6774e-01, -6.3226e-01,\n",
            "         9.7015e-02, -4.5782e-01, -4.1944e-01, -2.3530e-01,  3.0886e-01,\n",
            "         4.6736e-01,  7.7708e-01, -1.5254e-01, -6.0229e-01,  7.4539e-01,\n",
            "         2.1436e-01,  5.3897e-01,  1.1251e+00,  5.6357e-01, -1.2504e-01,\n",
            "         5.3288e-02,  1.0254e-01,  4.8041e-02,  3.5697e-01, -9.4904e-02,\n",
            "         7.6969e-01, -1.8288e-01, -1.7367e-01,  3.4809e-03,  3.0915e-01,\n",
            "        -1.2534e-01, -7.8552e-01,  1.3341e-01,  6.9190e-01,  6.4075e-02,\n",
            "         3.7330e-02,  4.9541e-01,  3.7623e-01, -1.3112e+00,  1.2760e-01,\n",
            "        -7.7008e-01,  2.5484e-01, -1.2322e-01, -3.3481e-01,  3.6384e-01,\n",
            "        -5.3885e-01, -7.2807e-01, -2.3416e-01, -2.2274e-01,  1.1214e-01,\n",
            "        -1.0241e-01,  1.1300e-01, -8.7083e-02, -2.3075e-01, -3.1436e-01,\n",
            "        -8.7888e-01,  1.1080e-01, -3.6912e-01, -4.1987e-01,  6.2177e-01,\n",
            "         1.4637e-01, -4.9562e-01, -1.9579e-01, -3.0407e-01,  2.5338e-01,\n",
            "         4.0595e-01, -2.8087e-02, -5.4146e-01, -7.5588e-01, -2.9321e-01,\n",
            "        -8.7009e-01, -3.3179e-01,  3.2996e-01, -7.1483e-01, -4.9683e-01,\n",
            "        -1.7298e-01, -3.3817e-01, -4.1590e-01, -7.6925e-01, -3.2865e-01,\n",
            "         8.5291e-01, -4.1119e-02,  7.3327e-01, -4.0590e-01, -2.3062e-01,\n",
            "         1.2592e-01,  5.0607e-01,  1.2938e+00,  3.3667e-01,  7.0343e-02,\n",
            "         8.7960e-01,  1.1598e-01,  3.0918e-01,  5.4559e-02,  5.1448e-01,\n",
            "        -1.8700e-01, -1.2885e-01,  4.6568e-03,  1.8686e-01, -9.7734e-02,\n",
            "        -9.4937e-02,  9.3892e-02, -9.7538e-02,  1.5127e-02, -1.8755e-01,\n",
            "        -8.8153e-01, -5.5694e-01,  3.4330e-01, -4.1345e-01, -3.8062e-01,\n",
            "         6.0924e-01,  2.1238e-02,  4.0184e-01,  2.5580e-02, -5.2646e-01,\n",
            "         3.6279e-02, -2.4123e-01, -5.3113e-01,  1.1812e-01, -8.4728e-02,\n",
            "         3.4132e-01,  2.3671e-01,  4.1450e-01, -3.6646e-01,  6.5945e-01,\n",
            "         4.0584e-01, -4.3870e-01,  3.7654e-01, -2.5959e-01, -4.3662e-02,\n",
            "         1.2272e-02, -7.6534e-03, -3.3701e-01, -3.5445e-02,  4.9498e-01,\n",
            "        -2.0730e+00,  2.2748e-02,  4.2498e-01, -1.6787e-01, -1.9596e-01,\n",
            "        -4.6230e-01, -4.3377e-01,  5.4708e-01, -2.1422e-01,  2.7463e-01,\n",
            "        -3.9096e-02,  2.1113e-02,  1.7429e-01, -2.7150e-01,  2.3586e-01,\n",
            "        -3.3721e-01, -3.1393e-01, -5.4904e-01, -6.4423e-02,  2.6483e-01,\n",
            "        -3.0179e-01,  3.6377e-01,  1.4786e-01, -5.1743e-01,  6.0602e-02,\n",
            "        -9.8478e-02, -1.4952e-01, -7.0311e-03,  1.4370e-01,  1.2668e+00,\n",
            "         1.6488e-01, -4.5125e-01, -1.2311e+00, -2.6906e-01, -4.2824e-01,\n",
            "        -1.4337e-01,  6.8537e-01,  3.0076e-03,  5.1297e-01,  1.8676e-01,\n",
            "        -2.2612e-01,  3.3915e-01,  4.0454e-01,  3.2680e-01,  3.3889e-01,\n",
            "         7.0015e-03, -3.6885e-01,  5.8279e-01, -1.6955e-01, -3.8411e-02,\n",
            "         1.0961e-02,  8.0209e-02, -7.4434e-02, -2.4708e-01,  1.8136e-01,\n",
            "        -2.7348e-01,  1.0227e-01,  2.5338e-01, -5.8794e-01,  1.5366e-01,\n",
            "         7.6965e-01, -7.8901e-01, -3.4075e-01,  3.0392e-01, -1.8823e-01,\n",
            "        -4.8222e-01, -4.9249e-01, -3.8248e-01, -2.4129e-01, -2.8826e-02,\n",
            "         1.3224e-01, -4.0198e-01, -3.5937e-01,  1.0449e+00, -8.2917e-01,\n",
            "        -1.8473e-01, -1.7667e-01, -2.9037e-03,  4.3533e-01, -3.7732e-01,\n",
            "         1.4997e-01,  1.8163e-01,  1.0420e+00,  4.1855e-01, -2.1847e-03,\n",
            "         8.6293e-01, -2.5067e-01, -1.1233e+00, -1.1669e-01,  4.9246e-01,\n",
            "        -9.2633e-01,  1.4591e-02,  4.2580e-01,  3.7530e-01,  1.6893e-01,\n",
            "        -4.7312e-01, -6.4982e-01, -9.0444e-02,  1.4013e-01, -9.0904e-01,\n",
            "        -3.9433e-01,  4.9604e-01,  7.2231e-01, -4.2066e-01, -2.5528e-01,\n",
            "         4.9038e-01,  5.3844e-01,  5.0100e-01, -1.4458e-01,  4.1919e-02,\n",
            "        -5.6600e-02, -7.8894e-02, -5.1846e-02,  1.5211e-01,  6.3977e-01,\n",
            "        -5.6543e-01, -2.5145e-01, -2.2133e-01,  1.7424e+00,  9.4680e-01,\n",
            "         3.6114e-01, -5.1005e-02,  2.3337e-01, -6.6699e-01,  3.2262e-01,\n",
            "         3.6952e-01, -4.1547e-01,  3.3970e-01,  1.7125e-01,  2.0278e-01,\n",
            "         5.6677e-01,  3.3933e-01,  9.0277e-01,  2.9786e-01,  3.8873e-01,\n",
            "        -1.8097e-01, -2.5575e-01,  2.8490e-01, -7.5303e-01,  9.9245e-01,\n",
            "        -1.8644e-01, -1.4631e-01,  1.1362e-01,  3.6830e-01, -2.3429e-01,\n",
            "         4.5992e-01, -1.9947e-01,  7.9125e-01, -3.1791e-01, -2.4878e-02,\n",
            "         5.2851e-01,  1.0308e-01, -8.1210e-03,  3.5878e-01, -4.7334e-01,\n",
            "        -2.3300e-01, -5.5349e-01, -3.7602e-01, -1.8613e-01, -2.2707e-02,\n",
            "         2.0446e-01, -2.9549e-01,  5.1838e-01,  9.2215e-01, -2.4830e-02,\n",
            "        -1.2123e-01,  1.8535e-01,  6.1488e-01,  2.1992e-01, -4.6773e-01,\n",
            "        -8.1752e-01, -4.8507e-01, -6.6923e-01, -3.0240e-01, -1.5375e-01,\n",
            "         7.6215e-01,  3.2594e-01,  3.9597e-01, -1.9574e-02,  4.1397e-01,\n",
            "         7.5509e-01,  1.0359e-01,  2.1144e-01, -4.3876e-01, -6.5247e-01,\n",
            "         1.0819e+00,  4.8273e-01, -2.6522e-01,  2.2197e-01,  3.4707e-01,\n",
            "         2.7004e-01, -1.5361e-01,  6.8803e-01,  3.0611e-01,  2.8955e-01,\n",
            "        -4.0963e-02, -3.5243e-01, -3.1283e+00, -1.2997e-01, -4.4401e-01,\n",
            "         3.3607e-01,  1.1775e-02,  1.4607e+00,  4.4724e-01, -1.7071e-01,\n",
            "         1.3347e-01, -4.9489e-01, -1.5198e-02,  2.8958e-01,  7.2463e-01,\n",
            "        -4.6353e-02,  1.1758e-01,  4.0441e-02,  7.2083e-01, -3.0782e-01,\n",
            "        -4.5482e-01, -3.6149e-01,  1.2624e-02, -5.2702e-04, -6.2769e-01,\n",
            "        -5.8167e-01, -1.0363e+00, -2.3529e-01,  3.0960e-01,  1.0636e-01,\n",
            "        -3.2829e-01,  5.4653e-01, -5.8227e-01,  5.8605e-01,  2.7849e-02,\n",
            "         2.9227e-02, -1.8170e-01, -5.3834e-01, -3.9084e-01,  4.8615e-01,\n",
            "        -1.6169e-01,  3.8368e-01, -3.7207e-01,  6.5664e-01, -3.9144e-01,\n",
            "         3.5679e-03, -1.8694e-01, -2.2022e-01,  6.2906e-01, -6.1602e-01,\n",
            "         7.2644e-01, -5.5125e-01,  2.9021e-02,  5.1185e-01, -2.5782e-01,\n",
            "        -2.3317e-01,  6.3185e-01,  1.8084e-01,  3.2656e-01, -7.5061e-03,\n",
            "        -4.8361e-02, -7.0478e-01,  2.1071e-01,  5.5516e-01,  3.3838e-01,\n",
            "        -1.4735e-01,  4.3450e-03, -1.0327e+00,  1.0349e-01, -3.7464e-01,\n",
            "        -1.7438e-01, -5.4789e-02,  1.3566e-01, -8.6843e-02,  4.1708e-01,\n",
            "        -1.4557e-01,  3.9958e-01,  1.4288e-01,  6.7622e-01,  9.5954e-01,\n",
            "         2.3274e-01, -5.4810e-01, -2.1633e-01,  1.7512e-01, -6.2762e-01,\n",
            "        -2.5462e-01, -3.4148e-01, -6.9767e+00,  3.0123e-01, -5.1974e-01,\n",
            "        -1.8170e-01,  1.7009e-01, -5.5510e-01, -4.4175e-01, -2.3672e-01,\n",
            "        -1.1669e-01, -3.4212e-01,  1.9540e-01, -6.1893e-02, -4.3286e-01,\n",
            "        -1.5890e-01,  2.5190e-01,  3.1068e-01], device='cuda:0',\n",
            "       grad_fn=<SelectBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnGYMFQoxlsT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7ea3234-a144-42d7-e2b6-a0a3eb9d8288"
      },
      "source": [
        "# print the first token embedding\n",
        "first_token_embedding = output[0][0][1]\n",
        "print(first_token_embedding.shape)\n",
        "print(first_token_embedding)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([768])\n",
            "tensor([-7.5579e-01,  2.3958e-02, -7.7775e-01, -2.9636e-02,  1.3142e-01,\n",
            "         3.4829e-01, -7.1905e-01,  2.2947e-01,  5.7406e-01, -5.2165e-01,\n",
            "        -6.3331e-01, -3.9139e-01,  6.1251e-01,  7.6919e-01,  2.4809e-01,\n",
            "         4.1138e-03,  3.1326e-01, -2.1136e-01,  3.4086e-01,  3.7136e-01,\n",
            "        -2.0355e-01, -2.5799e-01, -9.1278e-01,  1.2476e+00,  6.0668e-01,\n",
            "        -3.5893e-01,  2.8126e-01,  9.9478e-01,  7.2137e-01,  8.1404e-01,\n",
            "        -4.0528e-02, -2.9032e-01, -1.1082e-01, -4.8372e-01, -2.4480e-01,\n",
            "         2.6875e-01,  2.1412e-01,  2.4016e-01, -4.4493e-01,  5.2279e-01,\n",
            "         8.4595e-02, -4.5354e-01,  1.4433e-01,  2.5544e-01,  2.4262e-01,\n",
            "        -3.2311e-01,  4.1547e-01, -5.2544e-01,  6.2329e-02,  3.1465e-01,\n",
            "        -3.1489e-01, -1.1175e-01, -6.6004e-02, -3.6600e-01, -1.4372e-01,\n",
            "         7.6647e-01, -1.8431e-01,  9.4430e-01, -4.3552e-01,  4.6231e-01,\n",
            "         1.5902e-01,  9.4874e-01,  9.6713e-01, -5.6813e-01, -2.2341e-01,\n",
            "         6.0240e-01, -3.8815e-01, -5.6285e-01,  2.4936e-01,  5.1375e-01,\n",
            "        -6.5059e-02, -9.8402e-02, -9.3423e-01,  4.8592e-02, -1.1793e-01,\n",
            "         8.2361e-01, -7.5696e-03,  3.7472e-01,  7.3102e-02, -9.5219e-01,\n",
            "        -1.8445e-01,  1.1774e+00, -6.2466e-01,  7.8478e-01,  1.2433e-01,\n",
            "         3.2027e-01, -6.3096e-01,  6.2676e-01, -3.3143e-01, -1.3311e-01,\n",
            "        -4.0882e-01, -4.2772e-01, -3.2890e-01,  7.3149e-01,  5.2484e-01,\n",
            "        -8.7667e-01, -3.0802e-01, -5.0838e-02,  4.0435e-01,  3.3653e-01,\n",
            "         8.8067e-01, -3.5591e-01, -1.6515e-01,  2.8652e-01, -3.1887e-01,\n",
            "        -7.7040e-02,  2.6182e-01,  5.0252e-01, -7.5568e-02, -1.6181e+00,\n",
            "        -1.7690e-01, -1.6169e-01,  2.8581e-01, -3.5782e-02,  1.3690e-01,\n",
            "        -3.5499e-01,  4.7409e-01,  1.9309e-01, -2.5025e-01, -1.6127e-01,\n",
            "         1.4347e-01, -1.8475e-02, -1.6820e-01,  7.4182e-01,  1.4009e-01,\n",
            "        -4.9919e-01,  3.1766e-01, -3.2965e-02,  1.5798e-01, -1.2931e+00,\n",
            "         4.8440e-01,  6.4474e-01, -4.9068e-01, -9.0169e-01, -1.5733e-01,\n",
            "         6.3988e-01, -2.6536e-01, -6.6033e-01, -7.2560e-01, -1.1077e+00,\n",
            "         4.4554e-01, -1.0767e+00, -1.1159e+00, -3.1503e-01,  4.9594e-01,\n",
            "         7.2513e-02, -2.9556e-01, -5.1321e-01, -4.8530e-01,  4.8976e-01,\n",
            "        -3.7970e-01,  5.4998e-01, -2.3753e-01,  2.5858e-01,  6.3628e-01,\n",
            "         1.7703e-01, -1.0723e-01,  1.9740e-01,  3.0839e-01,  3.8947e-01,\n",
            "         1.1518e+00,  6.5227e-01, -2.2602e-01, -1.1098e-01, -4.0693e-01,\n",
            "         4.2404e-02,  5.2276e-01,  5.1700e-01, -3.1869e-01,  5.3066e-01,\n",
            "        -1.9332e-02, -1.7881e-01,  3.8810e-01,  4.7847e-01,  1.5245e+00,\n",
            "        -5.3385e-02, -8.0798e-02,  4.6481e-02, -5.3047e-01, -1.1216e-01,\n",
            "        -5.2007e-01,  8.7559e-01,  6.9970e-01, -1.1951e-01, -4.3267e-01,\n",
            "        -8.6452e-02,  1.2462e-01, -5.0172e-01,  1.3339e-01,  8.0366e-01,\n",
            "        -1.2819e-01, -4.4049e-02, -4.7944e-01,  6.6106e-01,  7.9243e-01,\n",
            "        -5.9389e-01, -5.4763e-01, -1.3315e+00,  2.9593e-01, -3.2937e-01,\n",
            "        -3.9985e-03, -2.7901e-01, -2.2611e-01, -1.9094e-02, -3.5794e-01,\n",
            "         1.4391e+00,  3.5870e-03,  2.2546e-01,  6.6420e-01,  5.1408e-01,\n",
            "        -7.2160e-01, -3.6530e-01, -4.6192e-01, -3.9431e-01,  8.1001e-01,\n",
            "        -4.8204e-01, -6.8769e-01, -4.7550e-01, -4.3504e-01,  7.0971e-02,\n",
            "         3.0640e-01,  1.7621e-01, -2.1800e-01,  3.7321e-01, -1.3769e-01,\n",
            "         8.5642e-01,  6.3037e-01,  7.7267e-01,  3.0206e-01, -8.4280e-01,\n",
            "        -9.2918e-02, -3.7193e-01,  6.4807e-01, -1.2160e-01, -8.0310e-02,\n",
            "        -8.5845e-01, -1.2045e+00, -8.9965e-01, -3.4390e-01,  2.6227e-01,\n",
            "        -3.5998e-01, -5.6127e-01,  8.0715e-01,  1.2090e+00, -3.1434e-01,\n",
            "         8.4145e-01,  5.4503e-01,  7.8816e-01, -8.9094e-01,  4.7546e-01,\n",
            "        -8.7448e-01, -1.1152e+00, -5.1337e-01, -1.0114e+00,  4.0044e-01,\n",
            "        -2.9186e-01, -2.8708e-02,  6.1337e-03, -5.3806e-01,  3.3051e-01,\n",
            "         1.9588e-01,  4.7652e-02,  1.4547e-01, -9.9830e-02, -6.5274e-02,\n",
            "        -2.5673e-01, -9.2170e-01,  1.3517e-01,  6.1111e-01,  9.6264e-02,\n",
            "        -3.1039e-01,  2.6228e-01,  6.4857e-01, -1.7777e+00, -9.7641e-02,\n",
            "        -9.0337e-01,  4.0951e-01, -2.8581e-02,  3.6815e-01,  5.1948e-01,\n",
            "         5.3429e-01, -2.0007e-01, -1.4536e-01,  3.7715e-01, -4.6373e-02,\n",
            "        -4.2703e-01, -2.5622e-01, -1.3034e-01, -4.7241e-01,  2.1532e-01,\n",
            "        -8.0314e-01, -2.0327e-01, -8.9991e-01, -6.5030e-02, -1.7009e-01,\n",
            "        -1.2793e-01,  2.4057e-01, -1.6480e-01, -1.6604e-01, -4.1553e-01,\n",
            "         4.5995e-01,  2.8742e-01, -1.1789e+00, -1.0946e-02,  1.8846e-01,\n",
            "         4.5453e-01, -2.5095e-01, -5.5865e-01, -2.6649e+00,  2.6969e-01,\n",
            "        -4.1416e-01,  2.6784e-01,  1.7676e-01, -6.1283e-02,  4.9533e-01,\n",
            "        -2.4463e-01, -6.9161e-01,  7.7503e-01,  2.0898e-01,  2.9298e-01,\n",
            "         1.1281e+00,  7.6863e-01,  8.4749e-01, -5.8232e-01,  1.7111e-01,\n",
            "        -4.5170e-01,  3.2564e-02,  2.7406e-01,  7.0483e-01, -2.9391e-01,\n",
            "         2.3521e-01, -9.9667e-01,  6.2946e-01,  1.1266e+00, -5.1235e-01,\n",
            "        -4.9473e-01, -1.4325e-01,  8.2357e-01,  4.5241e-01, -2.8098e-01,\n",
            "         4.2095e-01, -1.1891e-01,  5.9877e-01,  4.5590e-01, -6.5706e-01,\n",
            "         6.4994e-01,  4.9067e-01, -1.2173e-01, -2.3863e-01,  3.3450e-01,\n",
            "         4.2579e-02,  4.0564e-01,  1.4190e+00,  8.6934e-01,  8.5795e-01,\n",
            "        -1.8126e-01, -3.4971e-01,  2.2540e-01,  3.6141e-01, -9.5170e-01,\n",
            "         4.5317e-01, -2.4849e-01,  3.6198e-01,  2.3804e-01,  1.0829e-01,\n",
            "         2.4040e-01, -7.0064e-01, -2.7767e-01, -2.9448e-01,  7.3658e-01,\n",
            "        -5.3828e-01,  8.2276e-01,  2.2174e-01, -9.9759e-01, -4.3892e-01,\n",
            "        -9.3663e-01,  1.3230e+00,  7.8597e-02, -3.5327e-01,  2.8951e-01,\n",
            "        -1.7446e-01, -9.5192e-01, -1.2061e-01,  2.6587e-01,  1.0119e-01,\n",
            "        -7.7878e-03,  8.8973e-01, -5.0028e-01, -7.2835e-02, -4.1441e-01,\n",
            "        -4.4363e-01, -3.6679e-01, -9.1244e-02, -3.6047e-01,  1.2039e+00,\n",
            "        -1.4066e-01,  4.7227e-01, -1.9942e-01, -2.1893e-01,  1.2503e+00,\n",
            "         5.1468e-01,  2.8000e-01, -3.8830e-01, -1.0765e+00, -4.0494e-01,\n",
            "        -1.0021e+00, -6.2039e-01,  2.9730e-01, -7.0609e-01, -7.4908e-01,\n",
            "         6.3266e-01, -4.4253e-01,  2.1179e-01, -8.2250e-01, -6.9862e-01,\n",
            "         1.4905e-02,  1.4083e-01,  1.1654e+00, -6.3622e-02, -3.3514e-01,\n",
            "        -1.4709e-01,  1.6406e-01,  7.8049e-01,  3.0987e-01,  3.6382e-01,\n",
            "         7.4717e-01, -8.1403e-01,  2.0985e-01,  7.8414e-02,  1.0087e+00,\n",
            "        -1.3999e+00, -8.7751e-01,  3.4053e-01,  3.5067e-01, -2.6549e-01,\n",
            "        -6.2537e-01,  3.3558e-01, -1.6073e-01, -1.1090e-01, -4.2913e-01,\n",
            "        -5.1276e-01, -6.0696e-01,  9.6331e-02, -3.1913e-01, -9.8722e-01,\n",
            "         4.1341e-01, -2.0643e-01,  3.9270e-01, -3.3007e-01, -7.3136e-01,\n",
            "         5.9436e-01, -4.7749e-01, -2.8627e-01,  6.3482e-02, -8.7176e-01,\n",
            "        -2.1088e-01,  6.4913e-01,  2.0411e-01, -2.9045e-01,  6.1628e-01,\n",
            "         2.4411e-02, -4.2671e-01,  5.2390e-01, -8.4464e-01,  1.0768e+00,\n",
            "        -1.2369e-01, -3.8656e-01, -5.1781e-01, -5.3153e-01,  5.5383e-01,\n",
            "        -4.3058e-01, -4.1016e-01,  1.1831e-01,  5.8711e-01, -4.1230e-01,\n",
            "         4.6953e-01, -1.7801e-01, -2.1974e-01, -2.5065e-02,  3.8148e-01,\n",
            "         7.3953e-01, -4.1492e-01,  5.3098e-02, -1.7899e-01,  1.6205e-01,\n",
            "         2.6516e-01, -4.1911e-01, -1.0934e-01,  1.3077e-01,  1.0919e-01,\n",
            "        -4.1104e-01, -4.2077e-02,  4.1847e-01,  4.5172e-03, -8.9632e-01,\n",
            "         4.3297e-01, -2.1830e-01, -3.0017e-01,  3.5090e-01,  1.3168e+00,\n",
            "        -3.2973e-01, -1.4631e-01, -9.1597e-01, -3.2348e-01, -6.0591e-01,\n",
            "        -4.3481e-01,  6.7599e-01,  6.7682e-01,  5.4725e-01,  1.0592e-01,\n",
            "         3.3009e-01, -1.4647e-01,  5.3919e-03,  1.3073e-01,  4.3002e-01,\n",
            "        -3.0663e-01, -3.2981e-01,  5.1729e-01, -3.4230e-01,  5.6060e-01,\n",
            "         5.1760e-01, -8.2819e-01, -2.3114e-02, -1.7926e-01, -3.7366e-01,\n",
            "         1.0332e-01,  4.6724e-01, -2.6012e-02,  5.2583e-01,  4.1757e-01,\n",
            "         5.6341e-01, -4.3439e-01,  1.4549e-01, -2.5081e-01,  1.6973e-02,\n",
            "        -3.8401e-01, -1.2860e+00, -1.1017e-01,  6.1871e-01, -9.0477e-01,\n",
            "        -2.6528e-02, -2.2796e-01,  3.2476e-01,  4.4238e-01, -1.0794e+00,\n",
            "        -7.1177e-01,  1.4804e-01, -2.9651e-01,  2.1860e-02, -5.6377e-01,\n",
            "         5.4069e-01,  8.9640e-01,  6.2599e-01,  6.1226e-02, -4.4211e-02,\n",
            "         4.4387e-01, -3.8817e-01, -8.0305e-01,  6.4240e-02,  3.9842e-01,\n",
            "        -4.1481e-01, -2.7116e-03, -1.2440e-01,  6.6402e-02, -6.4106e-01,\n",
            "        -1.5687e-01, -6.7819e-01,  2.2474e-01,  3.1340e-01, -9.5013e-01,\n",
            "        -4.3932e-02,  2.0866e-01,  4.5051e-01, -6.3350e-01, -5.2007e-01,\n",
            "         6.1820e-01, -2.7517e-01,  1.0039e+00, -1.2099e-01,  2.4360e-01,\n",
            "         1.5898e-01,  3.5012e-01, -8.8799e-02, -3.2046e-01,  3.2541e-01,\n",
            "        -6.4385e-01, -1.6385e+00,  3.3690e-01,  1.0878e+00,  1.4571e+00,\n",
            "        -3.9602e-01, -8.2324e-02,  2.2201e-02, -6.5416e-01,  4.8163e-01,\n",
            "         4.3830e-01, -4.3688e-02,  2.7950e-01,  8.8643e-02, -1.3590e-01,\n",
            "         9.7724e-01,  2.2476e-01,  5.8296e-01, -5.0670e-01,  5.1539e-01,\n",
            "         1.5746e-01, -3.3100e-01,  6.3273e-01, -6.3246e-01,  8.6948e-01,\n",
            "         4.9798e-01,  2.7733e-02, -2.9509e-01, -1.6846e-01, -4.3624e-02,\n",
            "        -1.7918e-01,  4.4234e-02,  8.9065e-01, -2.4961e-01, -9.5628e-01,\n",
            "         5.1131e-01,  5.9029e-02,  1.3297e-01,  8.1743e-01, -4.8262e-01,\n",
            "        -7.3888e-01, -5.0400e-03,  5.4641e-02,  2.4298e-01,  4.4964e-02,\n",
            "        -1.3906e-01, -7.5923e-01,  1.4248e+00,  6.2550e-01, -3.2090e-01,\n",
            "         1.0988e-01, -6.0817e-01,  4.1140e-01,  8.4905e-01,  1.3297e-01,\n",
            "        -1.0750e+00, -4.8751e-01, -7.9981e-01, -1.4324e-01, -2.1591e-01,\n",
            "        -2.5953e-02,  5.7994e-01,  5.3541e-02,  3.5779e-02, -3.8467e-02,\n",
            "         1.3513e+00,  2.2628e-01,  4.3029e-01, -6.5107e-01, -6.0661e-01,\n",
            "         6.9332e-01,  4.7324e-01,  3.8017e-01,  9.4712e-01,  4.6594e-01,\n",
            "         5.3697e-01, -6.5988e-01,  1.4117e-01,  5.8670e-01,  3.6144e-01,\n",
            "         2.7407e-01, -2.9531e-02, -1.8049e+00,  2.6294e-02, -7.4932e-01,\n",
            "         7.9936e-01, -5.6755e-01,  1.5073e+00, -5.1896e-01,  1.3918e-01,\n",
            "        -1.4070e-01, -4.4652e-01,  3.0389e-01, -2.2233e-01,  6.4423e-03,\n",
            "        -5.0023e-01,  5.1452e-01,  1.8020e-01,  6.3382e-01, -3.1117e-01,\n",
            "        -4.9007e-01, -2.8290e-01,  3.7089e-01, -1.0454e-01, -2.6213e-01,\n",
            "        -6.9123e-01, -1.3140e+00, -1.3843e-01, -5.5058e-01, -3.8564e-01,\n",
            "        -6.1892e-01,  6.2172e-01, -2.7377e-01, -3.3997e-02, -3.5353e-01,\n",
            "         1.5236e-01, -1.0791e+00, -4.2750e-01, -1.5371e-02,  6.2064e-01,\n",
            "         3.2680e-01,  4.8749e-01, -7.1528e-01,  8.5601e-01, -8.1337e-03,\n",
            "        -1.7632e-01, -2.1718e-01, -2.4511e-01,  9.9526e-01, -4.2242e-01,\n",
            "         8.8224e-01, -1.0597e+00,  5.7377e-02,  1.5790e+00, -5.8768e-01,\n",
            "        -8.5232e-01,  8.1618e-01,  1.5336e-01,  7.5861e-01,  1.0465e-01,\n",
            "         8.3867e-02, -8.3664e-01, -2.3220e-01, -4.2425e-02,  6.6336e-01,\n",
            "         4.9654e-01, -2.1489e-01, -7.7419e-01, -6.9990e-01, -7.1318e-02,\n",
            "         3.4266e-01,  5.8334e-01, -1.1722e-01, -5.1892e-01, -2.2797e-01,\n",
            "         1.0589e-01, -1.1680e-01, -7.0993e-02,  5.0838e-02,  1.4081e+00,\n",
            "         5.9444e-01, -9.8024e-01,  2.6355e-01,  4.5494e-01,  1.7886e-01,\n",
            "        -2.5611e-01, -4.0798e-01, -4.0933e+00, -1.1572e-01, -8.3769e-01,\n",
            "        -1.6817e-01,  8.8353e-01, -3.3645e-01, -1.6190e-02, -8.1691e-01,\n",
            "         2.0342e-01,  1.1076e-01,  1.1823e-01, -4.3145e-01, -8.1011e-01,\n",
            "        -4.3465e-02,  4.0080e-01, -5.5262e-02], device='cuda:0',\n",
            "       grad_fn=<SelectBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cosine Similairty & Embeddings"
      ],
      "metadata": {
        "id": "BqwAucCQMmu_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_king = \"king\"\n",
        "text_man = \"man\"\n",
        "text_apple = \"apple\"\n",
        "\n",
        "# Function to convert text to embeddings\n",
        "def get_embeddings(text):\n",
        "  # Encode text and computer input_ids and attention mask\n",
        "  tokens = encode(text)\n",
        "  input_ids = tokens[\"input_ids\"].to(device)\n",
        "  attention_mask = tokens[\"attention_mask\"].to(device)\n",
        "  # Pass input_ids and attention mask to model\n",
        "  output = bert_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "  return output\n",
        "\n",
        "embed_king = get_embeddings(text_king)\n",
        "embed_man = get_embeddings(text_man)\n",
        "embed_apple = get_embeddings(text_apple)"
      ],
      "metadata": {
        "id": "qxXCNOH5HPKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_king"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Jbbt8acMuau",
        "outputId": "0d684b69-612e-4c0c-c692-2b10ddcf8e60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.1318,  0.1997,  0.0593,  ..., -0.1656, -0.0187, -0.0151],\n",
              "         [-0.1006, -0.0524,  0.0772,  ..., -0.2972, -0.1973, -0.6630],\n",
              "         [ 0.8228,  0.2570, -0.2939,  ..., -0.0014, -0.8282, -0.2063],\n",
              "         ...,\n",
              "         [-0.4145, -0.0221,  0.6602,  ..., -0.0110,  0.0749, -0.2281],\n",
              "         [-0.2734,  0.0148,  0.7546,  ..., -0.0276,  0.0481, -0.2719],\n",
              "         [-0.6719, -0.0510,  0.0872,  ...,  0.0179,  0.0716, -0.2357]]],\n",
              "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-0.7962, -0.2311,  0.4424,  0.6373, -0.3603, -0.1046,  0.8782,  0.1833,\n",
              "          0.2154, -0.9996,  0.1175,  0.2204,  0.9610, -0.2259,  0.8968, -0.3678,\n",
              "         -0.1455, -0.4299,  0.3994, -0.7417,  0.5365,  0.8699,  0.6029,  0.1351,\n",
              "          0.3400,  0.2738, -0.5180,  0.8717,  0.9329,  0.6474, -0.6485,  0.1758,\n",
              "         -0.9658, -0.2114,  0.3730, -0.9662,  0.1246, -0.7125, -0.0458,  0.0131,\n",
              "         -0.8082,  0.2329,  0.9899,  0.0665, -0.0993, -0.2937, -0.9993,  0.2260,\n",
              "         -0.7983, -0.3027, -0.2771, -0.5467,  0.1574,  0.3419,  0.3690,  0.3205,\n",
              "         -0.0206,  0.1386, -0.0897, -0.4822, -0.5136,  0.2723,  0.1127, -0.8444,\n",
              "         -0.2406, -0.5958, -0.1376, -0.2320, -0.0145, -0.0573,  0.7703,  0.1916,\n",
              "          0.3073, -0.6853, -0.4892,  0.1072, -0.3126,  1.0000, -0.3496, -0.9477,\n",
              "         -0.4389, -0.3059,  0.2304,  0.6420, -0.4732, -0.9999,  0.3521, -0.1071,\n",
              "         -0.9746,  0.1986,  0.1953, -0.1447, -0.5944,  0.1952, -0.1352, -0.0452,\n",
              "         -0.2637,  0.4262, -0.1476,  0.0279,  0.0481, -0.1894,  0.0593, -0.2743,\n",
              "          0.1031, -0.3256, -0.3603,  0.2080, -0.3712,  0.5667,  0.1713, -0.1830,\n",
              "          0.2345, -0.9366,  0.5523, -0.1461, -0.9560, -0.2763, -0.9704,  0.6296,\n",
              "         -0.0137, -0.1300,  0.9312,  0.4813,  0.1358,  0.0360,  0.4611, -1.0000,\n",
              "         -0.3529, -0.0120,  0.2196, -0.1074, -0.9414, -0.8985,  0.4821,  0.9214,\n",
              "          0.1030,  0.9848, -0.2242,  0.8757,  0.1214, -0.2201, -0.2942, -0.3495,\n",
              "          0.2105,  0.4023, -0.6114,  0.2044,  0.1015, -0.0740,  0.0110, -0.2068,\n",
              "          0.3765, -0.8885, -0.3453,  0.8882,  0.3391,  0.4272,  0.6132, -0.2065,\n",
              "         -0.2912,  0.7344,  0.0995,  0.3154,  0.0918,  0.3442, -0.0629,  0.4339,\n",
              "         -0.7757,  0.1679,  0.3323, -0.1229,  0.4770, -0.9426, -0.2802,  0.3347,\n",
              "          0.9687,  0.6941,  0.1763, -0.0712, -0.1211,  0.2241, -0.8922,  0.9331,\n",
              "         -0.1871,  0.1778,  0.5379, -0.1575, -0.7830, -0.4153,  0.7802, -0.0966,\n",
              "         -0.7655,  0.0410, -0.4001, -0.3144,  0.2531,  0.3859, -0.2417, -0.2763,\n",
              "          0.0299,  0.8400,  0.9368,  0.7622, -0.5360,  0.4059, -0.8587, -0.3472,\n",
              "          0.1009,  0.2246,  0.1396,  0.9801,  0.1337, -0.1062, -0.8709, -0.9688,\n",
              "          0.0782, -0.8267,  0.0253, -0.5853,  0.1803,  0.6831, -0.1883,  0.3276,\n",
              "         -0.9735, -0.6870,  0.3320, -0.1603,  0.3175, -0.1398, -0.1998, -0.2245,\n",
              "         -0.4550,  0.7228,  0.7859,  0.5514, -0.6445,  0.7746, -0.2189,  0.8438,\n",
              "         -0.5473,  0.9340, -0.1365,  0.4471, -0.8840,  0.3918, -0.8272,  0.2322,\n",
              "         -0.1331, -0.6139, -0.3209,  0.2138,  0.2310,  0.7604, -0.4876,  0.9938,\n",
              "          0.0738, -0.8931,  0.5078, -0.1464, -0.9620, -0.2501,  0.2419, -0.6754,\n",
              "         -0.2921, -0.2314, -0.8878,  0.8537,  0.1313,  0.9766,  0.1947, -0.8949,\n",
              "         -0.2445, -0.8005, -0.1704,  0.0084,  0.6135, -0.1047, -0.9250,  0.3826,\n",
              "          0.3403,  0.3129,  0.6816,  0.9910,  0.9962,  0.9488,  0.8208,  0.8468,\n",
              "         -0.6687,  0.2056,  0.9998, -0.3471, -0.9998, -0.8808, -0.4775,  0.3704,\n",
              "         -1.0000, -0.0309,  0.0249, -0.8669, -0.4206,  0.9507,  0.9775, -1.0000,\n",
              "          0.7585,  0.8897, -0.3944,  0.1540, -0.0700,  0.9393,  0.1684,  0.2028,\n",
              "         -0.1515,  0.2175,  0.1945, -0.8067,  0.4082,  0.4453,  0.2131,  0.1170,\n",
              "         -0.6375, -0.9071, -0.3204, -0.1141, -0.2061, -0.9062, -0.1341, -0.2536,\n",
              "          0.5664,  0.0886,  0.1198, -0.6942,  0.1477, -0.6090,  0.3122,  0.3902,\n",
              "         -0.8750, -0.5507, -0.3325, -0.4131,  0.3769, -0.8793,  0.9467, -0.2751,\n",
              "         -0.0530,  1.0000, -0.2091, -0.7933,  0.2252,  0.0895,  0.0097,  1.0000,\n",
              "          0.2878, -0.9450, -0.2623,  0.0995, -0.2576, -0.2425,  0.9939, -0.1683,\n",
              "          0.3802,  0.5084,  0.9278, -0.9685, -0.1468, -0.8453, -0.9185,  0.9300,\n",
              "          0.8822, -0.0246, -0.4643,  0.1319,  0.1263,  0.1994, -0.9261,  0.5828,\n",
              "          0.3802, -0.0999,  0.8308, -0.8564, -0.1773,  0.3113,  0.1063,  0.2906,\n",
              "         -0.2827,  0.4526, -0.2438,  0.1196, -0.2212,  0.3452, -0.9377, -0.1000,\n",
              "          1.0000,  0.0864, -0.5040, -0.0333, -0.0683, -0.2695,  0.2064,  0.3157,\n",
              "         -0.2567, -0.7497, -0.2035, -0.9013, -0.9597,  0.6911,  0.1619, -0.3156,\n",
              "          0.9959,  0.3112,  0.1527, -0.1788,  0.1012,  0.0941,  0.5148, -0.5780,\n",
              "          0.9262, -0.1412,  0.2412,  0.7255,  0.4333, -0.2423, -0.5663,  0.0575,\n",
              "         -0.8236, -0.0294, -0.9093,  0.9102, -0.3030,  0.2071,  0.0723, -0.0615,\n",
              "          1.0000,  0.5223,  0.4495, -0.5745,  0.8403, -0.5543, -0.6427, -0.3125,\n",
              "          0.0265,  0.4458, -0.2098,  0.2290, -0.9406, -0.3777, -0.1450, -0.9581,\n",
              "         -0.9793,  0.4469,  0.6998,  0.1177, -0.0524, -0.5345, -0.5024,  0.0742,\n",
              "         -0.0757, -0.8868,  0.5195, -0.1253,  0.4271, -0.1713,  0.2459, -0.5433,\n",
              "          0.7686,  0.6120,  0.1514, -0.0690, -0.7301,  0.6576, -0.7273,  0.3019,\n",
              "         -0.0931,  1.0000, -0.2921, -0.3079,  0.7037,  0.5699, -0.0609,  0.1161,\n",
              "         -0.3447,  0.1675,  0.4542,  0.5564, -0.8104, -0.2775,  0.3448, -0.5996,\n",
              "         -0.4704,  0.6769, -0.1673,  0.1006, -0.0231,  0.0671,  0.9974, -0.2560,\n",
              "         -0.0971, -0.4144, -0.0139, -0.2445, -0.5972,  0.9998,  0.3364, -0.1953,\n",
              "         -0.9710,  0.3780, -0.8628,  0.9905,  0.6698, -0.7798,  0.2827,  0.3535,\n",
              "         -0.1079,  0.6847, -0.2234, -0.2149,  0.0939,  0.1687,  0.9280, -0.3411,\n",
              "         -0.9160, -0.4833,  0.2311, -0.9023,  0.7065, -0.4249, -0.1486, -0.2166,\n",
              "          0.3929,  0.7968,  0.0768, -0.9415, -0.1265, -0.0571,  0.9368,  0.1056,\n",
              "         -0.2631, -0.8368, -0.3212, -0.2151,  0.5290, -0.8994,  0.9304, -0.9630,\n",
              "          0.2748,  0.9998,  0.2271, -0.5268,  0.1921, -0.3093,  0.1464,  0.1333,\n",
              "          0.4323, -0.8979, -0.1694, -0.1459,  0.2682, -0.1366,  0.2517,  0.5694,\n",
              "          0.1106, -0.2127, -0.4592, -0.0985,  0.3341,  0.6678, -0.2770, -0.0707,\n",
              "          0.0730, -0.0639, -0.8872, -0.1516, -0.1207, -0.9337,  0.5491, -1.0000,\n",
              "         -0.3571, -0.5329, -0.1633,  0.7641, -0.1589, -0.2201, -0.5893,  0.4392,\n",
              "          0.7946,  0.6821, -0.1835,  0.0280, -0.6001,  0.1475, -0.0470,  0.1838,\n",
              "          0.0995,  0.6250, -0.1220,  1.0000,  0.0862, -0.3401, -0.9525,  0.2321,\n",
              "         -0.2045,  0.9973, -0.8571, -0.8782,  0.1978, -0.3242, -0.7019,  0.1541,\n",
              "          0.0302, -0.5627,  0.0490,  0.9139,  0.8334, -0.2586,  0.2124, -0.2646,\n",
              "         -0.3956,  0.1010, -0.5062,  0.9723,  0.1520,  0.8114,  0.6897,  0.2640,\n",
              "          0.9080,  0.1477,  0.6620,  0.1119,  0.9999,  0.1800, -0.8694,  0.2777,\n",
              "         -0.9658, -0.1159, -0.9160,  0.2368,  0.0549,  0.7674, -0.2224,  0.9238,\n",
              "          0.4590,  0.1174,  0.0418,  0.6147,  0.2584, -0.8401, -0.9647, -0.9734,\n",
              "          0.1264, -0.3793, -0.0621,  0.2683,  0.1929,  0.2529,  0.2600, -0.9998,\n",
              "          0.8638,  0.3561, -0.4298,  0.9164,  0.0702,  0.1089,  0.1660, -0.9650,\n",
              "         -0.9464, -0.2854, -0.1979,  0.7319,  0.4535,  0.7636,  0.3412, -0.4369,\n",
              "          0.0544,  0.4900,  0.2102, -0.9801,  0.3131,  0.2971, -0.9346,  0.9102,\n",
              "         -0.4508, -0.1908,  0.5924,  0.3484,  0.9134,  0.7089,  0.4629,  0.1760,\n",
              "          0.4666,  0.8436,  0.9125,  0.9734,  0.3169,  0.6895,  0.3125,  0.2921,\n",
              "          0.0974, -0.8891,  0.0484, -0.1945, -0.0471,  0.2286, -0.1681, -0.9381,\n",
              "          0.3915, -0.1391,  0.4300, -0.3153,  0.1073, -0.3747, -0.1335, -0.6207,\n",
              "         -0.3699,  0.3778,  0.2876,  0.8636,  0.0934, -0.0768, -0.4952, -0.0911,\n",
              "          0.4127, -0.8584,  0.8418, -0.0089,  0.4413, -0.3880, -0.0457,  0.3125,\n",
              "         -0.2857, -0.3222, -0.2082, -0.6602,  0.7634,  0.1361, -0.3528, -0.4444,\n",
              "          0.5375,  0.2748,  0.8865,  0.4166,  0.2929,  0.0980, -0.1728,  0.1613,\n",
              "         -0.2167, -0.9998,  0.3911,  0.2125, -0.3272,  0.1248, -0.5085,  0.0644,\n",
              "         -0.9601, -0.0801, -0.3722, -0.3838, -0.4844, -0.4057,  0.2108,  0.2059,\n",
              "         -0.1088,  0.8234,  0.2449,  0.6713,  0.3334,  0.3364, -0.5699,  0.8260]],\n",
              "       device='cuda:0', grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Embedding of the word \"king\" at index 1 (since index 0 is reserved for [CLS] token)\n",
        "embed_king[0][0][1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "686rXc6hM5Va",
        "outputId": "de77f41e-0060-4feb-8d15-0a2c2a86d4e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-1.0063e-01, -5.2394e-02,  7.7210e-02, -3.3986e-01, -1.0123e-01,\n",
              "        -3.0228e-02,  1.7702e-01, -1.2432e-01, -4.0533e-01, -1.0918e+00,\n",
              "        -8.1277e-01,  1.5461e-01,  1.9359e-01,  4.6548e-01,  1.2872e-01,\n",
              "        -5.5175e-02, -1.7732e-02, -6.9906e-02,  3.8718e-01,  1.1769e-01,\n",
              "        -3.8392e-01,  1.5189e-01, -7.4889e-03, -9.0575e-02,  2.6472e-01,\n",
              "         5.8122e-01, -1.8173e-01,  1.4358e-01,  8.3955e-02,  4.5901e-01,\n",
              "         3.5349e-01, -6.7306e-01,  1.6623e-01,  2.6586e-01, -5.9197e-01,\n",
              "        -4.3626e-01,  5.0111e-01, -2.3916e-01, -6.2190e-01, -1.3644e-01,\n",
              "         4.4553e-01, -5.7102e-01,  7.1198e-01, -4.4230e-01,  2.3134e-01,\n",
              "         2.2516e-01,  1.0018e-01,  1.6920e-01,  2.4917e-01,  2.6961e-01,\n",
              "        -8.8481e-02,  2.2383e-01, -6.7094e-01,  2.2819e-01,  1.8186e-01,\n",
              "         6.0945e-02,  5.2540e-01,  3.3595e-01, -5.2457e-01, -2.2025e-01,\n",
              "         4.0992e-01, -8.3914e-01, -3.9756e-02, -5.4600e-01,  9.7653e-02,\n",
              "         6.9954e-01,  1.1494e+00,  2.9574e-01, -1.0256e+00,  1.0617e+00,\n",
              "        -1.2465e-01,  2.7267e-01,  4.7744e-01, -4.7300e-01,  7.2765e-01,\n",
              "        -5.9777e-02,  2.2545e-01, -1.1428e-01, -1.1398e-01, -6.3879e-01,\n",
              "         3.0547e-01,  6.0312e-01, -3.3178e-02,  6.1015e-01, -1.7905e-01,\n",
              "         6.3731e-02, -1.6092e-01, -3.8953e-01, -2.1471e-01,  7.4828e-01,\n",
              "        -3.9586e-01,  5.3149e-02, -8.5743e-02, -9.9372e-03, -2.2978e-01,\n",
              "        -1.0190e+00, -6.2711e-01,  4.6614e-01, -3.9287e-01, -1.2993e+00,\n",
              "        -3.7895e-01, -6.9495e-01,  5.0435e-01, -4.1752e-02, -9.7827e-01,\n",
              "         3.1060e-01, -2.2018e-01,  8.9480e-02, -1.9959e-01,  9.9549e-02,\n",
              "         9.7360e-02, -1.7495e-01,  3.3315e-01, -4.0842e-02, -2.5183e-02,\n",
              "         1.0413e+00, -6.5824e-01, -6.5225e-02, -6.0613e-01,  8.9612e-02,\n",
              "        -7.7541e-02, -6.4492e-02,  2.3346e-02,  7.2656e-01,  8.2801e-01,\n",
              "         4.7327e-02, -4.7434e-01, -2.1845e-01, -2.5852e-01, -3.2912e-01,\n",
              "         4.1004e-01,  9.8445e-02, -4.2225e-01,  3.6795e-01, -7.9281e-02,\n",
              "         7.2159e-01, -7.1630e-02, -3.8824e-01,  3.2138e-01, -3.8238e-01,\n",
              "         4.8917e-01,  5.3417e-02,  3.1346e-01, -2.6843e-01,  7.5929e-01,\n",
              "        -1.3142e-01, -1.4560e-01, -1.3523e-01,  8.8382e-02,  5.1046e-01,\n",
              "        -7.6064e-01,  7.6541e-01, -3.7686e-01, -3.9792e-01, -4.0092e-01,\n",
              "        -2.2023e-01, -1.9036e-01,  2.5523e-01, -1.5519e-02,  4.9717e-01,\n",
              "         3.8204e-01,  8.4253e-01, -3.4883e-01,  4.1897e-01, -1.7488e-02,\n",
              "        -5.1494e-01,  2.8548e-02,  1.1933e+00,  1.1339e-01, -8.0901e-01,\n",
              "        -6.8078e-01, -6.7870e-01,  1.0359e+00,  2.1374e-01, -3.0553e-01,\n",
              "         3.8300e-01,  1.6451e-01,  2.7271e-03, -2.0709e-01,  6.0575e-02,\n",
              "        -2.6370e-01,  1.0034e+00, -1.3057e-01, -1.6768e-01,  5.3397e-01,\n",
              "         4.2160e-01,  1.0551e+00, -6.1256e-01,  1.4873e-01,  2.7330e-01,\n",
              "        -4.6281e-01, -3.5475e-01,  3.2145e-01,  1.5608e-02,  4.6509e-01,\n",
              "        -2.2257e-01,  1.7945e-01, -4.1856e-01,  3.4674e-01,  6.1995e-02,\n",
              "         6.6730e-01, -2.0940e-01,  7.5660e-01, -1.9209e-02,  3.6604e-01,\n",
              "         6.2919e-01, -4.8206e-02, -4.2789e-01,  4.5375e-01, -1.0564e-01,\n",
              "        -7.2741e-01,  1.1683e+00,  4.1390e-02,  6.6036e-03,  3.9371e-01,\n",
              "        -2.0494e-01,  2.0151e-01,  1.5998e-03, -2.8878e-01,  3.4286e-01,\n",
              "         1.0302e+00, -9.4976e-02, -9.5721e-02,  2.6798e-01, -1.0864e+00,\n",
              "         5.9429e-01,  3.8553e-01, -5.3887e-01, -2.5031e-01,  4.4862e-01,\n",
              "        -3.2759e-01, -5.4759e-01, -5.4442e-01,  3.7009e-02, -1.2305e+00,\n",
              "         9.5795e-01, -1.6701e-01, -2.6101e-01,  6.9050e-03,  1.0437e-01,\n",
              "        -1.8194e-01,  9.7198e-02,  1.4394e-02,  8.3287e-01, -5.3005e-01,\n",
              "         6.6641e-01,  4.6126e-01, -2.8770e-01,  2.6097e-01,  1.6210e-01,\n",
              "         3.5393e-02, -7.1402e-01, -1.1349e-02,  3.6234e-02, -1.1365e+00,\n",
              "        -7.2965e-02, -1.7013e-01, -3.0995e-01,  1.1416e-01,  2.7503e-02,\n",
              "        -4.8356e-01,  1.7859e-01,  9.5786e-01, -1.2177e-02,  1.7701e-01,\n",
              "         4.3063e-01, -1.5770e-01,  3.9683e-01,  4.7720e-02, -5.7522e-02,\n",
              "        -4.2439e-01, -9.2840e-02, -6.9445e-02,  2.8967e-01, -1.6650e-01,\n",
              "         4.4627e-01,  7.2890e-01,  4.7099e-01,  2.8697e-01, -1.5089e-01,\n",
              "         5.2100e-01,  3.2639e-01, -4.7720e-01,  2.8689e-01,  8.4666e-01,\n",
              "        -1.3050e+00, -5.4000e-01, -4.6456e-01,  6.2318e-02, -2.0358e-01,\n",
              "         4.2722e-01,  5.8553e-01,  7.7103e-04, -4.5036e-01,  5.0791e-01,\n",
              "        -1.7135e-02, -2.0108e-01,  7.0513e-01, -1.9411e-01,  1.4782e-01,\n",
              "        -9.1780e-01,  5.2626e-01, -4.7229e-01, -4.1027e-01,  1.2512e+00,\n",
              "         3.0227e-01, -2.8126e-01,  6.4375e-01, -4.9269e+00, -4.4049e-01,\n",
              "         2.2971e-01, -3.1851e-01,  5.0283e-01, -6.0619e-01,  2.8903e-01,\n",
              "        -4.8676e-03,  2.5724e-01,  1.7254e-01,  9.7003e-01, -2.3184e-01,\n",
              "         7.3234e-01,  5.6317e-02,  2.4957e-01, -5.4193e-01, -4.5016e-01,\n",
              "        -5.2059e-01, -7.8734e-01,  5.5652e-01,  3.5713e-01, -1.6162e-01,\n",
              "         5.7293e-01, -4.6385e-01, -4.4385e-01,  1.2767e+00, -2.4490e-01,\n",
              "         6.4585e-01, -1.4855e+00, -2.3581e-01, -5.5359e-01, -6.0927e-02,\n",
              "         1.5604e-01, -7.9236e-01,  4.5296e-01,  3.8302e-01,  8.9131e-01,\n",
              "        -3.6638e-01,  1.3627e+00,  1.4494e-01,  6.2245e-01, -1.1037e+00,\n",
              "        -3.3570e-01,  3.7536e-01, -1.6487e-01, -1.7593e-01,  1.8980e-01,\n",
              "        -2.8371e-02,  2.2902e-02,  2.6058e-01, -7.8912e-03,  1.7742e-01,\n",
              "        -2.6185e-01, -6.0761e-01, -1.5455e-01,  7.6389e-01, -2.6717e-01,\n",
              "         9.8267e-01,  1.4641e-01, -1.5700e-01,  2.0460e-01, -2.6095e-01,\n",
              "        -8.0000e-02,  6.3531e-01, -1.0068e+00, -2.2288e-01,  1.3962e-01,\n",
              "         3.6091e-02,  3.8747e-03,  3.6481e-02, -1.5660e-01, -5.9478e-01,\n",
              "        -1.5907e-02, -1.5827e+00, -1.2926e+00,  6.4178e-04, -5.4977e-01,\n",
              "        -1.8843e-01,  4.5939e-01, -7.1883e-01, -1.2580e-03, -1.5179e-01,\n",
              "         4.5755e-01,  3.5149e-01, -9.6115e-02,  1.5494e-01, -8.8354e-02,\n",
              "        -5.7458e-01, -2.4279e-01,  4.4817e-01, -2.7988e-01,  1.5625e-01,\n",
              "         2.0782e-01,  9.3177e-02,  8.1886e-01, -2.3294e-01,  1.6176e-01,\n",
              "        -4.6750e-01,  7.9350e-02,  1.0422e-01, -3.3332e-01, -4.3404e-01,\n",
              "         4.3139e-01, -3.1657e-01, -2.6200e-01, -2.0190e-01, -1.1076e+00,\n",
              "         4.0032e-01, -9.1265e-01,  2.4077e-01,  3.4184e-01, -1.7350e-01,\n",
              "         2.5148e-01, -8.5702e-02, -6.3461e-01, -1.9182e-01, -9.9007e-02,\n",
              "         4.8346e-01, -2.8033e-02,  6.3713e-01, -4.6658e-01,  6.1217e-01,\n",
              "        -4.7276e-01, -1.6421e-01,  2.5036e-01,  9.5407e-02, -3.4117e-01,\n",
              "        -2.5316e-01, -4.3674e-01,  1.7346e-01,  4.1322e-01, -4.3279e-01,\n",
              "         4.3671e-02, -3.6298e-01, -5.5726e-01, -2.8385e-01, -3.0074e-01,\n",
              "        -5.6959e-01, -8.1541e-01,  3.9441e-01,  2.9459e-01, -1.5781e-01,\n",
              "        -6.6419e-01,  1.9677e-01,  7.4004e-02,  3.0239e-02, -6.0981e-01,\n",
              "         4.7656e-01,  4.1231e-01,  1.3980e+00,  3.6104e-01, -9.7775e-01,\n",
              "         9.7757e-01,  3.0614e-01, -2.3282e-01, -4.3378e-01,  2.0320e-01,\n",
              "         8.7544e-02,  6.0846e-01, -1.1403e-01,  4.9125e-01, -2.3647e-01,\n",
              "         1.4408e-01,  4.9926e-01,  1.2793e-01,  8.7281e-01, -1.5153e-01,\n",
              "         3.3249e-01, -4.1248e-01,  2.5289e-01,  6.0994e-01, -4.6877e-01,\n",
              "        -6.9221e-02, -1.7937e-01, -1.2513e-01,  4.4583e-01, -2.2443e-01,\n",
              "        -9.0553e-01,  3.4631e-02, -1.2059e-01, -9.8589e-02,  3.1289e-02,\n",
              "         3.4659e-01, -5.3399e-01,  5.8802e-01,  1.8510e-01, -1.5104e-01,\n",
              "        -2.0536e-01,  1.8124e-01,  7.8299e-01, -2.6902e-01, -5.0682e-01,\n",
              "        -4.9917e-01, -2.1559e-01, -4.2205e-01, -2.4730e-01,  5.3466e-01,\n",
              "         1.0414e+00,  6.6865e-01,  8.1647e-01, -5.0389e-02, -6.5294e-01,\n",
              "         5.6572e-02,  9.4247e-02,  1.2728e-01,  1.5460e-01, -1.6974e-01,\n",
              "         3.0230e-01, -2.5506e-01, -2.1037e-01,  7.4202e-02, -3.3805e-01,\n",
              "        -1.9331e-01, -1.2269e-01, -1.7753e-01,  7.8971e-01,  5.5163e-01,\n",
              "         2.0584e-02,  1.5997e-01, -4.7163e-01, -6.5854e-01, -8.2958e-01,\n",
              "        -1.4280e-01, -5.2122e-02, -3.2656e-01, -4.2775e-01, -1.9930e-01,\n",
              "        -3.0176e-01,  3.7357e-01, -8.2328e-02,  2.6841e-01,  3.8828e-01,\n",
              "        -5.9485e-02,  4.6196e-01,  4.0530e-01, -2.0292e-02, -5.4397e-01,\n",
              "        -1.2246e-01, -8.6142e-01, -2.2527e-01,  3.9499e-01, -5.6000e-01,\n",
              "        -4.4759e-01, -2.7679e-02, -1.6214e-01,  6.0302e-01,  3.9468e-01,\n",
              "         4.7401e-01,  1.5839e+00,  9.2493e-01, -3.3973e-01,  4.5323e-01,\n",
              "         7.4383e-01,  5.6620e-01,  4.7211e-01, -1.6449e+00,  7.3975e-02,\n",
              "         4.2480e-01,  2.3669e-01,  4.0130e-02, -8.7222e-01,  8.8671e-01,\n",
              "         7.3624e-02,  1.9635e-01, -5.1120e-01,  4.1162e-02, -1.6534e-01,\n",
              "         5.5580e-02,  4.2744e-01,  3.1861e-01, -7.3594e-01,  2.5531e-03,\n",
              "        -3.4076e-01,  5.4366e-01, -7.7975e-01, -1.8832e-01,  5.2314e-01,\n",
              "        -4.5163e-01, -4.9284e-01, -8.1314e-02, -2.6155e-01,  8.4112e-01,\n",
              "        -4.0935e-01,  2.0194e-01,  6.7623e-01, -3.0550e-01, -8.7449e-02,\n",
              "        -4.5859e-01, -2.8883e-01, -6.7660e-01,  4.4950e-02,  5.3997e-01,\n",
              "         4.9905e-01,  8.0884e-01, -3.2290e-01,  2.9589e-01,  2.0715e-01,\n",
              "         2.0547e-01, -6.5160e-01, -1.3694e-01, -1.7905e-01, -1.4164e-01,\n",
              "        -1.2337e-01, -2.6687e-01, -1.2747e-01,  4.6221e-01,  5.4819e-02,\n",
              "        -2.7218e-01, -9.4932e-02, -9.6987e-02, -2.3630e-01, -5.0420e-01,\n",
              "        -1.2698e-02,  8.5567e-01, -3.8883e-01,  1.4059e-02,  5.6940e-01,\n",
              "        -6.6377e-01, -1.8258e-01,  3.0829e-01, -9.0176e-02, -5.8254e-01,\n",
              "         4.8402e-01,  4.7929e-01,  6.3193e-01,  6.5665e-01, -1.4196e+00,\n",
              "         9.5078e-02,  6.9298e-01,  6.2292e-01,  1.4915e-01, -5.5783e-01,\n",
              "        -4.5300e-01,  2.2888e-01,  8.6615e-02, -2.2640e-01,  6.6527e-01,\n",
              "         5.2996e-02, -2.2037e-01, -4.4620e-01,  5.3510e-01, -5.9835e-02,\n",
              "         2.8537e-01, -4.9198e-01,  2.4114e-01, -5.6944e-01,  3.8084e-01,\n",
              "        -6.0621e-01,  2.9669e-01,  5.8079e-01, -3.2116e-02,  2.0508e-01,\n",
              "        -8.1369e-01,  1.3595e-01,  3.2118e-01,  4.8978e-01, -4.9362e-02,\n",
              "        -6.7201e-01, -7.0313e-01,  9.1342e-02,  2.2653e-01, -5.1464e-01,\n",
              "         3.2691e-01,  9.8904e-02, -2.6538e-01,  1.1927e-01,  1.2865e-01,\n",
              "        -2.3699e-01, -3.8960e-01, -2.2443e-01, -1.8549e-02, -2.8186e-02,\n",
              "         3.8577e-01,  7.2268e-01, -5.4747e-01, -6.1025e-01,  6.8416e-02,\n",
              "         1.6098e-01, -4.0144e-01, -7.0150e-01,  1.8892e-01, -9.2404e-02,\n",
              "         6.2020e-02, -2.6881e-01,  2.5029e-01, -2.1565e-01,  1.1842e-01,\n",
              "         2.9087e-01, -8.5545e-02, -2.5702e-01,  3.1466e-02, -3.6456e-01,\n",
              "        -5.6049e-01, -5.6106e-01,  3.6609e-01,  8.4541e-03, -4.8851e-01,\n",
              "         9.4664e-02,  1.3545e-01,  1.2025e+00,  8.1331e-01, -1.4355e-01,\n",
              "        -3.1002e-02, -4.3207e-02, -2.0266e-01, -1.0731e-01,  3.3055e-01,\n",
              "         2.0405e-01, -7.3994e-01, -7.6300e-01,  6.5409e-01,  3.8107e-02,\n",
              "        -2.8056e-01, -8.4904e-02, -6.0464e-02, -9.5176e-01,  3.2681e-01,\n",
              "        -2.0573e-01, -2.7637e-01,  1.6498e-02,  2.0097e-01, -2.1159e-03,\n",
              "         2.9057e-01,  3.5870e-01,  1.8111e-01, -3.9732e-01, -7.1979e-01,\n",
              "         6.1592e-01,  7.2268e-03,  8.5351e-01, -2.9770e-01,  4.6130e-01,\n",
              "        -3.6511e-01, -1.7466e-01, -7.1493e-02, -4.9004e-01,  2.1388e-01,\n",
              "        -6.4397e-01,  2.5773e-02, -8.1239e-01, -2.9857e-01,  3.4168e-01,\n",
              "         1.9970e-02, -1.0093e-02, -2.7917e-01, -1.1855e+00, -4.6426e-01,\n",
              "         4.2989e-01,  1.6490e-01, -3.5680e-01,  1.5997e-01, -2.1145e-01,\n",
              "         7.3155e-02,  2.0203e-02,  1.1278e-01,  1.1683e-01,  1.3056e-01,\n",
              "        -2.9724e-01, -1.9733e-01, -6.6299e-01], device='cuda:0',\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "jPvYrtO_JoV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# helper function\n",
        "def compute_cosine_similarity(a, b):\n",
        "    # Move array to CPU memory, detach from computational graph which requires_grad, convert to numpy & reshape\n",
        "    a = a.cpu().detach().numpy().reshape(1, -1)\n",
        "    b = b.cpu().detach().numpy().reshape(1, -1)\n",
        "    cos_sim = cosine_similarity(a, b)\n",
        "    return cos_sim"
      ],
      "metadata": {
        "id": "ESLy5TIZIYUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare cosine similarity between (king, man, appple) embeddings\n",
        "\n",
        "king_apple_similarity = compute_cosine_similarity(embed_king[0][0][1], embed_apple[0][0][1])\n",
        "print(f\"Similarity between King & Apple is {king_apple_similarity * 100}\")\n",
        "\n",
        "king_man_similarity = compute_cosine_similarity(embed_king[0][0][1], embed_man[0][0][1])\n",
        "print(f\"Similarity between King & Man is {king_man_similarity * 100}\")\n",
        "\n",
        "man_apple_similarity = compute_cosine_similarity(embed_man[0][0][1], embed_apple[0][0][1])\n",
        "print(f\"Similarity between Man & Apple is {man_apple_similarity * 100}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VIRvzq9jMQsN",
        "outputId": "89cbc4df-c55b-4eb2-891c-cc5e464d6877"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity between King & Apple is [[66.72247]]\n",
            "Similarity between King & Man is [[74.478874]]\n",
            "Similarity between Man & Apple is [[65.9001]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "king_queen_similarity = compute_cosine_similarity(get_embeddings(\"queen\")[0][0][1], embed_king[0][0][1])\n",
        "print(f\"Similarity between King & Queen is {king_queen_similarity * 100}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtDMa1SyOFtQ",
        "outputId": "3e442737-0d3c-40b2-de51-499742b11186"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity between King & Queen is [[86.43858]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FVdZjX5vHMg"
      },
      "source": [
        "## **Exercise1**\n",
        "Given the following sentences:\n",
        "\n",
        "sent_1 = \"A research team at Zewail City conducted an information retrieval course in English\"\n",
        "\n",
        "sent_2 = \"Natural Language Processing is one of the most popular fields in 2024\"\n",
        "\n",
        "1. Compute the embeddings of these sentences\n",
        "2. Check whether the embeddings of the word \"research\" are equal by computing the cosine similarity.\n",
        "3. Compute the cosine similarity between the two sentences/embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aivkp1p_jOsA"
      },
      "source": [
        "\n",
        "sent_1 = \"A research team at Zewail City conducted an information retrieval course in English\"\n",
        "sent_2 = \"Natural Language Processing is one of the most popular research topics in 2024\"\n",
        "\n",
        "# Before passing the embedding to compute cosine similarity score,\n",
        "# you have to convert them from tensor to numpy array as follows:\n",
        "# input1 = token1_embeddging.detach().numpy()\n",
        "# input2 = token2_embeddging.detach().numpy()\n",
        "# cosine_score = compute_cosine_similarity(input1, input2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71_fMCB2FXIG"
      },
      "source": [
        "### **References**\n",
        "\n",
        "\n",
        "*  [Hugging Face: State-of-the-Art Natural Language Processing in ten lines of TensorFlow 2.0](https://blog.tensorflow.org/2019/11/hugging-face-state-of-art-natural.html).\n",
        "*   [BERT Fine-Tuning Tutorial with PyTorch.](https://mccormickml.com/2019/07/22/BERT-fine-tuning/)\n",
        "\n"
      ]
    }
  ]
}