{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bosy-Ayman/IR/blob/main/week_9_Term_Representations%26Embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIOx_DMOhRPW"
      },
      "source": [
        "\n",
        "\n",
        "The **learning outcomes** of the this notebook are:\n",
        "\n",
        "\n",
        "*   Get words embeddings using Skip Gram / Glove / Cbow model.\n",
        "*   Compare the similarity of words embeddings of those models.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Last Lab Recap:**\n",
        "\n",
        "\n",
        "A **word embedding** is a distributed (dense) vector representation for each word of a vocabulary. It is capable of capturing semantic and syntactic properties of the input texts .\n",
        "\n",
        "Interestingly, even arithmetic operations on the word vectors are meaningful: e.g. King - Queen = Man - Woman.\n",
        "\n",
        "**The two most popular approaches to learn a word embedding from raw text are:**\n",
        "\n",
        "- **Skip-Gram Negative Sampling** by Mikolov et al. (2013) (Word2Vec)\n",
        "- **Global Word Vectors** by Pennington, Socher, and Manning (2014) (GloVe)."
      ],
      "metadata": {
        "id": "8MaXgr7SwC1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fhfVWgS-p94P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Skip-Gram Word2Vec â€” Intuition**\n",
        "\n",
        "<img src= \"https://raw.githubusercontent.com/gaoisbest/NLP-Projects/master/0_Word2vec/Word2vec_Skip_gram.png\" alt=\"drawing\" width=\"750\"/>\n",
        "\n",
        "**The skip-gram model**  aims to generate word embeddings, which are dense vector representations, where similar words are closer together in the vector space.\n",
        "\n",
        "In the skip-gram model, each word is represented by a **unique vector**. The model takes a target word as input and tries to predict the context words within a certain window size. By training the model on large amounts of text data.\n",
        "\n"
      ],
      "metadata": {
        "id": "_MlKbDdwHhqa"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqwYZsMoPKyV"
      },
      "source": [
        "- Skip-gram, **predicts the context words** given a target word. It takes a target word as input and aims to predict the surrounding context words.\n",
        "\n",
        "- The **input** to the skip-gram model is a **target word**, and the **output** is a **set of context words**.\n",
        "\n",
        "- The target word is typically represented as **a one-hot encoded vector**.\n",
        "\n",
        "- Skip-gram learns to map the input target word to the context words by **training a neural network**, with an input layer, a hidden layer, and an output layer.\n",
        "\n",
        "- During training, the weights of the neural network are adjusted to minimize the prediction error between the predicted context words and the actual context words.\n",
        "\n",
        "- Skip-gram is computationally **more expensive** compared to CBOW as it requires making predictions for each context word, but it tends to **perform better on larger datasets** and captures well the semantics of infrequent words.\n",
        "\n",
        "\n",
        "\n",
        "###  **In a two-gram example:**\n",
        "\n",
        "$$\\underbrace{\\textrm{The quick}}_{\\textrm{left } n}\\underbrace{\\textrm{ brown }}_{target} \\underbrace{\\textrm{for jumps}}_{\\textrm{right } n}$$\n",
        "\n",
        "<img src=\"https://nbviewer.jupyter.org/github/DSKSD/DeepNLP-models-Pytorch/blob/master/images/01.skipgram-prepare-data.png\">\n",
        "\n",
        "\n",
        "\\begin{equation}\\large \\prod P(V_c|V_t) \\rightarrow \\sum logP(V_c|V_t) \\rightarrow \\sum log\\frac{\\exp^{u_{t,c}}}{\\sum_{k=1}^{|V|}\\exp^{u_{t,k}}}\\end{equation}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Skip - Gram / Example1:\n",
        "\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Example corpus\n",
        "corpus = [\n",
        "    ['apple', 'banana', 'orange', 'fruit'],\n",
        "    ['car', 'bus', 'train', 'vehicle'],\n",
        "    ['cat', 'dog', 'pet', 'animal'],\n",
        "    ['computer', 'keyboard', 'mouse', 'technology']\n",
        "]"
      ],
      "metadata": {
        "id": "_fLLHuvPsiTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train skip-gram model\n",
        "model = Word2Vec(sentences=corpus,\n",
        "                 sg=1,                  #indicates the skip-gram model\n",
        "                 vector_size=100,       #each word will be represented as a vector of 100 dimensions.\n",
        "                 window=2,              #size of the context window/ model will consider two words to the left/right of the target word.\n",
        "                 min_count=1,           #minimum frequency count of words required to be included in the vocabulary.\n",
        "                 workers=4,             #utilizing multiple CPU cores.\n",
        "                 epochs=20)             #Each epoch goes through the entire dataset once\n",
        "\n",
        "# Get word embeddings\n",
        "word_embeddings = model.wv\n",
        "\n",
        "# Example usage\n",
        "print(word_embeddings['apple'])  # Get the embedding vector for the word 'apple'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLrk3gHkspvy",
        "outputId": "da652ac9-8a01-4b17-b460-f9eaf1a46d60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-7.1907109e-03  4.2323531e-03  2.1672160e-03  7.4456348e-03\n",
            " -4.8877355e-03 -4.5667640e-03 -6.0967854e-03  3.3006475e-03\n",
            " -4.5027859e-03  8.5193124e-03 -4.2901128e-03 -9.1109248e-03\n",
            " -4.8195980e-03  6.4187679e-03 -6.3732183e-03 -5.2567786e-03\n",
            " -7.3009222e-03  6.0254098e-03  3.3571543e-03  2.8429865e-03\n",
            " -3.1370206e-03  6.0324781e-03 -6.1509013e-03 -1.9880428e-03\n",
            " -5.9824740e-03 -9.9266437e-04 -2.0254641e-03  8.4903855e-03\n",
            "  7.5093449e-05 -8.5716275e-03 -5.4278509e-03 -6.8761348e-03\n",
            "  2.6935276e-03  9.4525181e-03 -5.8173346e-03  8.2637137e-03\n",
            "  8.5330810e-03 -7.0587411e-03 -8.8811945e-03  9.4740056e-03\n",
            "  8.3817160e-03 -4.6954509e-03 -6.7312834e-03  7.8448728e-03\n",
            "  3.7651756e-03  8.0972137e-03 -7.5730658e-03 -9.5266094e-03\n",
            "  1.5822783e-03 -9.8044751e-03 -4.8857513e-03 -3.4626198e-03\n",
            "  9.6272007e-03  8.6237462e-03 -2.8371222e-03  5.8311294e-03\n",
            "  8.2379077e-03 -2.2612293e-03  9.5285922e-03  7.1662096e-03\n",
            "  2.0402709e-03 -3.8501143e-03 -5.0848583e-03 -3.0499455e-03\n",
            "  7.8895157e-03 -6.1856946e-03 -2.9150832e-03  9.1907429e-03\n",
            "  3.4593639e-03  6.0749399e-03 -8.0355583e-03 -7.5438456e-04\n",
            "  5.5267136e-03 -4.7153072e-03  7.4778539e-03  9.3181664e-03\n",
            " -4.0731792e-04 -2.0667964e-03 -5.8832997e-04 -5.7908199e-03\n",
            " -8.3928751e-03 -1.5072633e-03 -2.5585594e-03  4.3801470e-03\n",
            " -6.8665743e-03  5.4121306e-03 -6.7443629e-03 -7.8211976e-03\n",
            "  8.4756855e-03  8.9174425e-03 -3.4788670e-03  3.4905465e-03\n",
            " -5.7963775e-03 -8.7500447e-03 -5.5177943e-03  6.7479298e-03\n",
            "  6.4184139e-03  9.4394321e-03  7.0539704e-03  6.7582801e-03]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(word_embeddings['banana'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZMaHSmGcJwM",
        "outputId": "fe70e8e5-2574-4eae-f4d6-5fbfe7ee99cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 7.6964465e-03  9.1211973e-03  1.1375165e-03 -8.3227847e-03\n",
            "  8.4265033e-03 -3.6962493e-03  5.7440475e-03  4.3913233e-03\n",
            "  9.6879499e-03 -9.2945024e-03  9.2078838e-03 -9.2838397e-03\n",
            " -6.9083665e-03 -9.1018500e-03 -5.5472087e-03  7.3707197e-03\n",
            "  9.1653587e-03 -3.3237957e-03  3.7222467e-03 -3.6280856e-03\n",
            "  7.8822952e-03  5.8679120e-03  2.2938245e-06 -3.6323101e-03\n",
            " -7.2242739e-03  4.7704726e-03  1.4511441e-03 -2.6123982e-03\n",
            "  7.8363018e-03 -4.0482013e-03 -9.1480883e-03 -2.2557415e-03\n",
            "  1.2593227e-04 -6.6412883e-03 -5.4865568e-03 -8.5008750e-03\n",
            "  9.2294523e-03  7.4265879e-03 -2.9409389e-04  7.3685488e-03\n",
            "  7.9536289e-03 -7.8567682e-04  6.6098371e-03  3.7691286e-03\n",
            "  5.0779395e-03  7.2527640e-03 -4.7397711e-03 -2.1858814e-03\n",
            "  8.7507279e-04  4.2361082e-03  3.3043462e-03  5.0948756e-03\n",
            "  4.5897844e-03 -8.4397728e-03 -3.1843686e-03 -7.2352518e-03\n",
            "  9.6811568e-03  5.0071445e-03  1.7087515e-04  4.1145915e-03\n",
            " -7.6567428e-03 -6.2962854e-03  3.0757205e-03  6.5357955e-03\n",
            "  3.9496026e-03  6.0204817e-03 -1.9861481e-03 -3.3440858e-03\n",
            "  2.0708842e-04 -3.1928876e-03 -5.5190222e-03 -7.7895871e-03\n",
            "  6.5378612e-03 -1.0896276e-03 -1.8920322e-03 -7.8054951e-03\n",
            "  9.3380632e-03  8.6654624e-04  1.7711744e-03  2.4889917e-03\n",
            " -7.3892009e-03  1.6377114e-03  2.9765060e-03 -8.5684434e-03\n",
            "  4.9566436e-03  2.4325727e-03  7.4991914e-03  5.0439099e-03\n",
            " -3.0309143e-03 -7.1637281e-03  7.0975279e-03  1.9005951e-03\n",
            "  5.1992782e-03  6.3811010e-03  1.9119413e-03 -6.1276588e-03\n",
            " -5.4939160e-06  8.2693556e-03 -6.0989344e-03  9.4390120e-03]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(word_embeddings.similarity('apple', 'banana'))  # Compute similarity between two words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVMHyyo6s19z",
        "outputId": "015b8d62-7651-4605-b240-98a8c7ac2959"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-0.098008834\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(word_embeddings.most_similar('car'))  # Find most similar words to 'car'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvjPWSVhcc2V",
        "outputId": "795e0d19-8a34-40cc-80a9-860795db9ced"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('keyboard', 0.17271797358989716), ('pet', 0.16696742177009583), ('cat', 0.11119077354669571), ('bus', 0.1094270721077919), ('technology', 0.07960997521877289), ('computer', 0.04131349176168442), ('vehicle', 0.037712957710027695), ('apple', 0.013257332146167755), ('mouse', 0.008347253315150738), ('animal', -0.0058680507354438305)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Skip - Gram / Example2:\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models import KeyedVectors #utility class in Gensim models such as (Word2Vec, GloVe, FastText, etc.)"
      ],
      "metadata": {
        "id": "8dUP3zQz6jlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "collection = [[\"king\", \"is\", \"to\", \"queen\", \"as\", \"man\", \"is\", \"to\", \"woman\"],\n",
        "    [\"london\", \"is\", \"the\", \"capital\", \"of\", \"egland\"],\n",
        "    [\"paris\", \"is\", \"the\", \"capital\", \"of\", \"france\"]]"
      ],
      "metadata": {
        "id": "VLXnXXuE-9kv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Word2Vec model\n",
        "model = Word2Vec(collection, vector_size=100, #each word will be represented as a 100-dimensional vector.\n",
        "                             window=5,        #model will look at 5 words before and 5 words after the target word in each sentence.\n",
        "                             min_count = 1,   #All words in the corpus are considered, regardless of their frequency.\n",
        "                             workers=4)       #Training is performed using 4 CPU cores in parallel."
      ],
      "metadata": {
        "id": "FQX-jFEe6now"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Access word vectors\n",
        "man_vector = model.wv['man']\n",
        "print(man_vector)"
      ],
      "metadata": {
        "id": "gy1kwjVEDMSt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09c3fadf-5199-4997-c124-6a4b0a2a275c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 7.0887972e-03 -1.5679300e-03  7.9474989e-03 -9.4886590e-03\n",
            " -8.0294991e-03 -6.6403709e-03 -4.0034545e-03  4.9892161e-03\n",
            " -3.8135587e-03 -8.3199050e-03  8.4117772e-03 -3.7470020e-03\n",
            "  8.6086961e-03 -4.8957514e-03  3.9185942e-03  4.9220170e-03\n",
            "  2.3926091e-03 -2.8188038e-03  2.8491246e-03 -8.2562361e-03\n",
            " -2.7655398e-03 -2.5911583e-03  7.2490061e-03 -3.4634031e-03\n",
            " -6.5997029e-03  4.3404270e-03 -4.7448516e-04 -3.5975564e-03\n",
            "  6.8824720e-03  3.8723124e-03 -3.9002013e-03  7.7188847e-04\n",
            "  9.1435025e-03  7.7546560e-03  6.3618720e-03  4.6673026e-03\n",
            "  2.3844899e-03 -1.8416261e-03 -6.3712932e-03 -3.0181051e-04\n",
            " -1.5653884e-03 -5.7228567e-04 -6.2628710e-03  7.4340473e-03\n",
            " -6.5914928e-03 -7.2392775e-03 -2.7571463e-03 -1.5154004e-03\n",
            " -7.6357173e-03  6.9824100e-04 -5.3261113e-03 -1.2755442e-03\n",
            " -7.3651113e-03  1.9605684e-03  3.2731986e-03 -2.3138524e-05\n",
            " -5.4483581e-03 -1.7260861e-03  7.0849168e-03  3.7362587e-03\n",
            " -8.8810492e-03 -3.4135508e-03  2.3541022e-03  2.1380198e-03\n",
            " -9.4640078e-03  4.5711659e-03 -8.6569972e-03 -7.3870681e-03\n",
            "  3.4831120e-03 -3.4709584e-03  3.5644709e-03  8.8940905e-03\n",
            " -3.5743224e-03  9.3204249e-03  1.7110384e-03  9.8477742e-03\n",
            "  5.7050432e-03 -9.1494834e-03 -3.3277308e-03  6.5301750e-03\n",
            "  5.6027793e-03  8.7055154e-03  6.9261026e-03  8.0388878e-03\n",
            " -9.8230084e-03  4.2988253e-03 -5.0300765e-03  3.5123860e-03\n",
            "  6.0566878e-03  4.3921317e-03  7.5123594e-03  1.4977157e-03\n",
            " -1.2649416e-03  5.7684006e-03 -5.6395675e-03  3.8591625e-05\n",
            "  9.4565870e-03 -5.4812501e-03  3.8142789e-03 -8.1130210e-03]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Find similar words to 'king' and 'Paris' based on their vector representations learned during training\n",
        "similar_to_man = model.wv.most_similar(positive=['man'], topn=3)\n",
        "similar_to_paris = model.wv.most_similar(positive=['paris'], topn=3)\n",
        "\n",
        "# Print results\n",
        "print(\"Words similar to man:\", similar_to_man)\n",
        "print(\"Words similar to Paris:\", similar_to_paris)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OA8wBH1ADc3V",
        "outputId": "ffa538ba-9447-420f-dec1-7cecc03be570"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words similar to man: [('queen', 0.12813478708267212), ('as', 0.10944210737943649), ('egland', 0.1088901236653328)]\n",
            "Words similar to Paris: [('capital', 0.14593380689620972), ('man', 0.05048326402902603), ('the', 0.04167982563376427)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate word similarity (cosine similarity)\n",
        "king_queen_sim = model.wv.similarity('king', 'queen')\n",
        "paris_london_sim = model.wv.similarity('paris', 'london')\n",
        "\n",
        "print(\"Similarity between king and queen:\", king_queen_sim)\n",
        "print(\"Similarity between Paris and London:\", paris_london_sim)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T91GW62qDvLu",
        "outputId": "6fba6c0c-45f3-4d69-c3de-69ad935403b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity between king and queen: -0.076410025\n",
            "Similarity between Paris and London: 0.008817988\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.most_similar('king')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AtK4-P-u_JrA",
        "outputId": "4a26dbc6-468c-4fe9-c28c-5b41727c12ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('to', 0.25285014510154724),\n",
              " ('of', 0.1372225135564804),\n",
              " ('france', 0.04404586926102638),\n",
              " ('paris', 0.012902338989078999),\n",
              " ('the', 0.006779309827834368),\n",
              " ('man', -0.001192279625684023),\n",
              " ('egland', -0.025496706366539),\n",
              " ('is', -0.04117467626929283),\n",
              " ('queen', -0.07641002535820007),\n",
              " ('woman', -0.10560528188943863)]"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.most_similar(positive=[\"king\", \"woman\"], negative=[\"man\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQzApY309mE1",
        "outputId": "7398b667-950b-41d6-910f-9105bcfb8f5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('to', 0.20373943448066711),\n",
              " ('of', 0.10580124706029892),\n",
              " ('is', 0.06748092174530029),\n",
              " ('paris', -0.021386904641985893),\n",
              " ('capital', -0.03990182653069496),\n",
              " ('france', -0.04878506809473038),\n",
              " ('london', -0.06293847411870956),\n",
              " ('the', -0.10544881224632263),\n",
              " ('egland', -0.1370125561952591),\n",
              " ('queen', -0.15060536563396454)]"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2- CBOW**\n",
        "\n",
        "<img src= \"https://raw.githubusercontent.com/gaoisbest/NLP-Projects/master/0_Word2vec/Word2vec_CBOW.png\" width=750>\n",
        "   \n",
        "**CBOW (Continuous Bag of Words):**\n",
        "\n",
        "- CBOW predicts the target word based on its **surrounding** context words. It treats **the context words as input** and tries to predict the target word in the middle.\n",
        "\n",
        "- The input to the CBOW model is a set of context words, and the output is the target word.\n",
        "- The context words are usually represented as one-hot encoded vectors.\n",
        "- CBOW learns to map the input context words to the target word by training a neural network. The network typically consists of an **input layer, a hidden layer, and an output layer**.\n",
        "- CBOW is **computationally efficient** as it aggregates the context words to predict the target word, making it **faster to train** compared to skip-gram.\n",
        "- CBOW is more suitable for **small to medium**-sized datasets and performs well when the frequency of words is balanced."
      ],
      "metadata": {
        "id": "HD2Jk6y0vcE_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src= \"https://machinelearninginterview.com/wp-content/uploads/2019/02/CBOW_eta_Skipgram.png\" width=750>\n",
        "\n",
        "\n",
        "### Comparisons of CBOW and Skip-gram\n",
        "- speed\n",
        "    - cbow: **faster**, skip-gram: **slower**,\n",
        "- infrequent words\n",
        "    - cbow: **bad**, skip-gram: **better**, [why](https://stats.stackexchange.com/questions/180548/why-is-skip-gram-better-for-infrequent-words-than-cbow)?\n",
        "- training data\n",
        "    - cbow: **smaller datasets**, skip-gram: **larger datasets**.\n",
        "    - CBOW smoothes over a lot of the distributional information (by treating an entire context as one observation), useful for **smaller datasets**. Skip-gram treats each context-target pair as a new observation, and tends to do better when **larger datasets**.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### The key difference from the skip-gram example is that we set **sg=0** to indicate that we want to train a CBOW model."
      ],
      "metadata": {
        "id": "NqSiGBqJyGOM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cbow - Example1:\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Example corpus\n",
        "corpus = [\n",
        "    ['apple', 'banana', 'orange', 'fruit'],\n",
        "    ['car', 'bus', 'train', 'vehicle'],\n",
        "    ['cat', 'dog', 'pet', 'animal'],\n",
        "    ['computer', 'keyboard', 'mouse', 'technology']\n",
        "]"
      ],
      "metadata": {
        "id": "gIvphE7ivDDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train CBOW model\n",
        "model = Word2Vec(sentences=corpus,\n",
        "                 sg=0,                 #indicates the Cbow model\n",
        "                 vector_size=100,\n",
        "                 window=2,\n",
        "                 min_count=1,\n",
        "                 workers=4,\n",
        "                 epochs=20)\n",
        "\n",
        "# Get word embeddings\n",
        "word_embeddings = model.wv\n",
        "\n",
        "# Example usage\n",
        "print(word_embeddings['apple'])  # Get the embedding vector for the word 'apple'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rleUxyiWvK51",
        "outputId": "10f73816-86b9-4db9-87ee-ea7de6dae968"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-7.1908082e-03  4.2325766e-03  2.1617643e-03  7.4409596e-03\n",
            " -4.8884735e-03 -4.5629623e-03 -6.0976543e-03  3.2995059e-03\n",
            " -4.5010662e-03  8.5241403e-03 -4.2908899e-03 -9.1059618e-03\n",
            " -4.8195105e-03  6.4160223e-03 -6.3749091e-03 -5.2612112e-03\n",
            " -7.3046330e-03  6.0232445e-03  3.3607727e-03  2.8443786e-03\n",
            " -3.1379368e-03  6.0308878e-03 -6.1522140e-03 -1.9797122e-03\n",
            " -5.9856055e-03 -9.9437870e-04 -2.0221702e-03  8.4889755e-03\n",
            "  7.5262338e-05 -8.5753957e-03 -5.4289075e-03 -6.8746139e-03\n",
            "  2.6933060e-03  9.4538145e-03 -5.8159786e-03  8.2620177e-03\n",
            "  8.5308459e-03 -7.0604258e-03 -8.8847755e-03  9.4710710e-03\n",
            "  8.3781090e-03 -4.6936437e-03 -6.7288997e-03  7.8465613e-03\n",
            "  3.7670960e-03  8.0935322e-03 -7.5730742e-03 -9.5260078e-03\n",
            "  1.5809102e-03 -9.8075606e-03 -4.8859590e-03 -3.4576450e-03\n",
            "  9.6275695e-03  8.6216936e-03 -2.8370069e-03  5.8246474e-03\n",
            "  8.2333554e-03 -2.2656773e-03  9.5290951e-03  7.1597849e-03\n",
            "  2.0436456e-03 -3.8520712e-03 -5.0809868e-03 -3.0489094e-03\n",
            "  7.8873020e-03 -6.1896397e-03 -2.9144010e-03  9.1941496e-03\n",
            "  3.4554109e-03  6.0745417e-03 -8.0356384e-03 -7.4996200e-04\n",
            "  5.5229845e-03 -4.7156662e-03  7.4792085e-03  9.3217818e-03\n",
            " -4.0701058e-04 -2.0623379e-03 -5.8938423e-04 -5.7898350e-03\n",
            " -8.3902637e-03 -1.5102023e-03 -2.5583210e-03  4.3815826e-03\n",
            " -6.8688611e-03  5.4111355e-03 -6.7427289e-03 -7.8176744e-03\n",
            "  8.4709749e-03  8.9175524e-03 -3.4795166e-03  3.4906180e-03\n",
            " -5.7948907e-03 -8.7490426e-03 -5.5153659e-03  6.7448141e-03\n",
            "  6.4187814e-03  9.4419774e-03  7.0571536e-03  6.7571830e-03]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(word_embeddings.similarity('apple', 'banana'))  # Compute similarity between two words\n",
        "print(word_embeddings.most_similar('car'))  # Find most similar words to 'car'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUJx-omDvS-v",
        "outputId": "36195c3b-4b05-4974-dddb-064ff4586e56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-0.098114535\n",
            "[('keyboard', 0.17277397215366364), ('pet', 0.1669520139694214), ('cat', 0.11120960861444473), ('bus', 0.10944785177707672), ('technology', 0.07959192991256714), ('computer', 0.04128841683268547), ('vehicle', 0.037712957710027695), ('apple', 0.013238158077001572), ('mouse', 0.008328912779688835), ('animal', -0.005901669152081013)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3- Glove**\n",
        "\n",
        "**GloVe (Global Vectors for Word Representation):**\n",
        "\n",
        "- GloVe aims to capture the global **co-occurrence statistics** of words in a corpus. It considers the overall word co-occurrence patterns, rather than just the local context of individual words.\n",
        "- GloVe constructs a co-occurrence matrix that represents the frequency of word co-occurrences in the corpus. Each entry in the matrix represents **how often two words co-occur** within a certain context window.\n",
        "\n",
        "- The key idea behind GloVe is that the ratio of co-occurrence probabilities of two words should encode meaningful information about their relationship.\n",
        "\n",
        "- The model optimizes the embeddings by minimizing the difference between the dot product of word vectors and the logarithm of the co-occurrence probabilities.\n"
      ],
      "metadata": {
        "id": "_6QtlCwPfX1_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CRVdfNCi_B7"
      },
      "source": [
        "# **Glove using FLAIR Library**\n",
        "\n",
        "We will use [FLAIR: An Easy-to-Use Framework for State-of-the-Art NLP](https://www.aclweb.org/anthology/N19-4010/). FLAIR make it easy to get words and documents embeddings using a huge number of SOTA models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Uxlak0PvNVU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11dfac23-d007-4911-9579-f20886230104"
      },
      "source": [
        "#install FLAIR\n",
        "!pip install flair"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: flair in /usr/local/lib/python3.10/dist-packages (0.13.1)\n",
            "Requirement already satisfied: boto3>=1.20.27 in /usr/local/lib/python3.10/dist-packages (from flair) (1.34.88)\n",
            "Requirement already satisfied: bpemb>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from flair) (0.3.5)\n",
            "Requirement already satisfied: conllu>=4.0 in /usr/local/lib/python3.10/dist-packages (from flair) (4.5.3)\n",
            "Requirement already satisfied: deprecated>=1.2.13 in /usr/local/lib/python3.10/dist-packages (from flair) (1.2.14)\n",
            "Requirement already satisfied: ftfy>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from flair) (6.2.0)\n",
            "Requirement already satisfied: gdown>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from flair) (4.7.3)\n",
            "Requirement already satisfied: gensim>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from flair) (4.3.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from flair) (0.20.3)\n",
            "Requirement already satisfied: janome>=0.4.2 in /usr/local/lib/python3.10/dist-packages (from flair) (0.5.0)\n",
            "Requirement already satisfied: langdetect>=1.0.9 in /usr/local/lib/python3.10/dist-packages (from flair) (1.0.9)\n",
            "Requirement already satisfied: lxml>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from flair) (4.9.4)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.10/dist-packages (from flair) (3.7.1)\n",
            "Requirement already satisfied: more-itertools>=8.13.0 in /usr/local/lib/python3.10/dist-packages (from flair) (10.1.0)\n",
            "Requirement already satisfied: mpld3>=0.3 in /usr/local/lib/python3.10/dist-packages (from flair) (0.5.10)\n",
            "Requirement already satisfied: pptree>=3.1 in /usr/local/lib/python3.10/dist-packages (from flair) (3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from flair) (2.8.2)\n",
            "Requirement already satisfied: pytorch-revgrad>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from flair) (0.2.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from flair) (2023.12.25)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from flair) (1.2.2)\n",
            "Requirement already satisfied: segtok>=1.5.11 in /usr/local/lib/python3.10/dist-packages (from flair) (1.5.11)\n",
            "Requirement already satisfied: sqlitedict>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from flair) (2.1.0)\n",
            "Requirement already satisfied: tabulate>=0.8.10 in /usr/local/lib/python3.10/dist-packages (from flair) (0.9.0)\n",
            "Requirement already satisfied: torch!=1.8,>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from flair) (2.2.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.63.0 in /usr/local/lib/python3.10/dist-packages (from flair) (4.66.2)\n",
            "Requirement already satisfied: transformer-smaller-training-vocab>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from flair) (0.4.0)\n",
            "Requirement already satisfied: transformers[sentencepiece]<5.0.0,>=4.18.0 in /usr/local/lib/python3.10/dist-packages (from flair) (4.38.2)\n",
            "Requirement already satisfied: urllib3<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from flair) (1.26.18)\n",
            "Requirement already satisfied: wikipedia-api>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from flair) (0.6.0)\n",
            "Requirement already satisfied: semver<4.0.0,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from flair) (3.0.2)\n",
            "Requirement already satisfied: botocore<1.35.0,>=1.34.88 in /usr/local/lib/python3.10/dist-packages (from boto3>=1.20.27->flair) (1.34.88)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from boto3>=1.20.27->flair) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from boto3>=1.20.27->flair) (0.10.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bpemb>=0.3.2->flair) (1.25.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bpemb>=0.3.2->flair) (2.31.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from bpemb>=0.3.2->flair) (0.1.99)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.13->flair) (1.14.1)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy>=6.1.0->flair) (0.2.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown>=4.4.0->flair) (3.13.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown>=4.4.0->flair) (1.16.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown>=4.4.0->flair) (4.12.3)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim>=4.2.0->flair) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim>=4.2.0->flair) (6.4.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.10.0->flair) (2023.6.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.10.0->flair) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.10.0->flair) (4.11.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.10.0->flair) (24.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2.3->flair) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2.3->flair) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2.3->flair) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2.3->flair) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2.3->flair) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.2.3->flair) (3.1.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from mpld3>=0.3->flair) (3.1.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.2->flair) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.2->flair) (3.4.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.5.0->flair) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.5.0->flair) (3.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.5.0->flair) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.5.0->flair) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.5.0->flair) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.5.0->flair) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.5.0->flair) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.5.0->flair) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.5.0->flair) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.5.0->flair) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.5.0->flair) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.5.0->flair) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.5.0->flair) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.5.0->flair) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch!=1.8,>=1.5.0->flair) (12.4.127)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]<5.0.0,>=4.18.0->flair) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]<5.0.0,>=4.18.0->flair) (0.4.3)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]<5.0.0,>=4.18.0->flair) (3.20.3)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]<5.0.0,>=4.18.0->flair) (0.29.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown>=4.4.0->flair) (2.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->mpld3>=0.3->flair) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->bpemb>=0.3.2->flair) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bpemb>=0.3.2->flair) (3.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bpemb>=0.3.2->flair) (2024.2.2)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests->bpemb>=0.3.2->flair) (1.7.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.8,>=1.5.0->flair) (1.3.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->transformers[sentencepiece]<5.0.0,>=4.18.0->flair) (5.9.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAoO4bAhvXAs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ce556c3-79f1-4624-f854-d94911cc33df"
      },
      "source": [
        "from flair.data import Sentence # represent a sentence\n",
        "from flair.embeddings import WordEmbeddings\n",
        "\n",
        "from termcolor import colored #add color to text output\n",
        "\n",
        "# initialize embedding by specifying which model we want to use\n",
        "glove_embedding = WordEmbeddings('glove')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-21 11:09:20,047 https://flair.informatik.hu-berlin.de/resources/embeddings/token/glove.gensim.vectors.npy not found in cache, downloading to /tmp/tmpuurl4w23\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 153M/153M [00:07<00:00, 22.7MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-21 11:09:27,522 copying /tmp/tmpuurl4w23 to cache at /root/.flair/embeddings/glove.gensim.vectors.npy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-21 11:09:27,828 removing temp file /tmp/tmpuurl4w23\n",
            "2024-04-21 11:09:28,312 https://flair.informatik.hu-berlin.de/resources/embeddings/token/glove.gensim not found in cache, downloading to /tmp/tmp3vztd5k_\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20.5M/20.5M [00:01<00:00, 12.8MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-21 11:09:30,424 copying /tmp/tmp3vztd5k_ to cache at /root/.flair/embeddings/glove.gensim\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-21 11:09:30,451 removing temp file /tmp/tmp3vztd5k_\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HaPZTi1vYVD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49b43e5e-8489-4676-f933-30c3acdd897b"
      },
      "source": [
        "# create sentence. Sentence class holds all meta related to a text\n",
        "glove_sentence = Sentence('We are travelling to Italy to watch a famous play')\n",
        "print(glove_sentence)\n",
        "print(glove_sentence.tokens)\n",
        "\n",
        "#Sentence will split our sentence to tokens. Let's access the first token\n",
        "print(glove_sentence[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence[10]: \"We are travelling to Italy to watch a famous play\"\n",
            "[Token[0]: \"We\", Token[1]: \"are\", Token[2]: \"travelling\", Token[3]: \"to\", Token[4]: \"Italy\", Token[5]: \"to\", Token[6]: \"watch\", Token[7]: \"a\", Token[8]: \"famous\", Token[9]: \"play\"]\n",
            "Token[0]: \"We\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoHKuN3Z4jz0"
      },
      "source": [
        "#print each token embedding. We will get empty vectors because we did not get the embeddings yet\n",
        "for token in glove_sentence:\n",
        "    print(colored(token,attrs=['bold']))\n",
        "\n",
        "    #print the embedding for each token\n",
        "    print(token.embedding)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zC20I21xyOM0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0765b0e3-6f78-46dc-e01d-69f41f224d14"
      },
      "source": [
        "# embed a sentence using glove.\n",
        "glove_embedding.embed(glove_sentence)\n",
        "\n",
        "# now check out the embedded tokens.\n",
        "for token in glove_sentence:\n",
        "    print(colored(token,attrs=['bold']))\n",
        "    #print the embedding for each token\n",
        "    print(token.embedding)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token[0]: \"We\"\n",
            "tensor([-0.1779,  0.6267,  0.4787, -0.5530, -0.8493, -0.0708, -0.3472,  0.4628,\n",
            "         0.1261, -0.2488,  0.4688,  0.0836,  0.5606, -0.2193,  0.0156, -0.5581,\n",
            "        -0.2074,  0.9123, -1.2034,  0.3011,  0.4668,  0.4830, -0.1020, -0.5680,\n",
            "        -0.0271,  0.4057, -0.1406, -0.5548,  0.0946, -0.6221, -0.3034,  0.6064,\n",
            "         0.0498,  0.2220,  0.4855,  0.1763, -0.0905,  0.5371,  0.2755, -0.7883,\n",
            "        -0.7095, -0.1668,  0.1121, -0.4849, -0.6664,  0.0840,  0.3289, -0.4585,\n",
            "        -0.3721, -1.5315,  0.1299, -0.2409, -0.1722,  1.3740, -0.2231, -2.6150,\n",
            "         0.3520,  0.3360,  1.6117,  0.9295, -0.3753,  0.8203, -1.0677, -0.4533,\n",
            "         1.2332,  0.2375,  0.6352,  0.8286, -0.1744, -0.5853,  0.5634, -0.7309,\n",
            "         0.3081, -1.0888,  0.4614,  0.0454, -0.1783, -0.0541, -0.8831,  0.0339,\n",
            "         0.6308, -0.1974, -0.9905,  0.2002, -1.9266, -0.2588,  0.1037, -0.3413,\n",
            "        -0.9351, -0.5467, -0.4017, -0.3778, -0.0658, -0.1384, -0.9187, -0.0556,\n",
            "        -0.0806, -0.1953,  0.2008,  1.0953])\n",
            "Token[1]: \"are\"\n",
            "tensor([-0.5153,  0.8319,  0.2246, -0.7387,  0.1872,  0.2602, -0.4256,  0.6712,\n",
            "        -0.3108, -0.6127,  0.0895, -0.2401,  1.1878,  0.6761, -0.0229, -0.9253,\n",
            "         0.0712,  0.3884, -0.4292,  0.3714,  0.3267,  0.4314,  0.8749,  0.3401,\n",
            "        -0.2319, -0.4114,  0.4906, -0.3291, -0.4911, -0.1899,  0.3341, -0.2124,\n",
            "        -0.3839, -0.0805,  1.1161,  0.2362,  0.3133,  0.4929,  0.1000, -0.1513,\n",
            "        -0.1418, -0.2802, -0.2388, -0.3549,  0.1828, -0.1913,  0.6054,  0.0746,\n",
            "        -0.2073, -0.6097,  0.1991, -0.5702, -0.1743,  1.4419, -0.2502, -1.8648,\n",
            "         0.4167, -0.2461,  1.5010,  0.8741, -0.6714,  1.2762, -0.2721,  0.1758,\n",
            "         1.2242,  0.2824,  0.6237,  0.6395,  0.3691, -0.8468, -0.3227, -0.6715,\n",
            "        -0.1963, -0.4079, -0.2097, -0.1962,  0.0419,  0.5397, -1.1105, -0.3952,\n",
            "         0.6659, -0.2330, -1.0820,  0.0465, -2.0993, -0.2849,  0.0800, -0.1296,\n",
            "        -0.3001, -0.4676, -0.8183, -0.0485, -0.3223, -0.3201, -1.1207, -0.0568,\n",
            "        -0.7300, -1.2024,  1.1304,  0.3479])\n",
            "Token[2]: \"travelling\"\n",
            "tensor([-0.2403, -0.3514,  0.0460,  0.1393,  0.7243,  0.3459, -0.2276,  0.9463,\n",
            "         0.1174, -0.2170, -0.1862, -0.0962,  0.8667,  0.0921, -0.0319, -0.6446,\n",
            "         0.0721, -0.2683, -0.2630,  0.0493,  0.6990,  0.7463,  0.3406, -0.5361,\n",
            "         0.1351, -0.2971, -0.1881,  0.3009,  0.8656,  0.2575, -1.2645, -0.1430,\n",
            "        -0.5173, -0.1686,  0.2274,  0.4881,  0.0112,  0.4954, -0.0910,  0.2980,\n",
            "        -1.0946,  0.1273,  0.3685,  0.1331,  0.5175, -0.0921,  0.3587,  0.0947,\n",
            "         0.3740, -0.0479,  0.3277,  0.5067,  0.4356,  0.8495, -0.0402, -0.7825,\n",
            "        -0.5635,  0.0624,  0.8372,  0.0777,  0.3513,  0.6986, -0.1294,  0.3292,\n",
            "        -0.2570,  0.7771, -0.4407, -0.3510,  0.3413,  0.2825, -0.9943, -0.8431,\n",
            "         0.5462, -0.6543,  0.2517,  0.1179,  0.9765,  0.0137,  0.0176, -0.6579,\n",
            "         0.4505,  0.5378,  0.1635, -0.4675, -0.6158, -0.2066, -0.4133, -0.5959,\n",
            "        -0.7081, -0.7501, -0.2510, -0.1775,  0.6747, -0.8007, -0.1990,  0.0420,\n",
            "        -0.0565,  0.7173,  0.8409, -0.3422])\n",
            "Token[3]: \"to\"\n",
            "tensor([-1.8970e-01,  5.0024e-02,  1.9084e-01, -4.9184e-02, -8.9737e-02,\n",
            "         2.1006e-01, -5.4952e-01,  9.8377e-02, -2.0135e-01,  3.4241e-01,\n",
            "        -9.2677e-02,  1.6100e-01, -1.3268e-01, -2.8160e-01,  1.8737e-01,\n",
            "        -4.2959e-01,  9.6039e-01,  1.3972e-01, -1.0781e+00,  4.0518e-01,\n",
            "         5.0539e-01, -5.5064e-01,  4.8440e-01,  3.8044e-01, -2.9055e-03,\n",
            "        -3.4942e-01, -9.9696e-02, -7.8368e-01,  1.0363e+00, -2.3140e-01,\n",
            "        -4.7121e-01,  5.7126e-01, -2.1454e-01,  3.5958e-01, -4.8319e-01,\n",
            "         1.0875e+00,  2.8524e-01,  1.2447e-01, -3.9248e-02, -7.6732e-02,\n",
            "        -7.6343e-01, -3.2409e-01, -5.7490e-01, -1.0893e+00, -4.1811e-01,\n",
            "         4.5120e-01,  1.2112e-01, -5.1367e-01, -1.3349e-01, -1.1378e+00,\n",
            "        -2.8768e-01,  1.6774e-01,  5.5804e-01,  1.5387e+00,  1.8859e-02,\n",
            "        -2.9721e+00, -2.4216e-01, -9.2495e-01,  2.1992e+00,  2.8234e-01,\n",
            "        -3.4780e-01,  5.1621e-01, -4.3387e-01,  3.6852e-01,  7.4573e-01,\n",
            "         7.2102e-02,  2.7931e-01,  9.2569e-01, -5.0336e-02, -8.5856e-01,\n",
            "        -1.3580e-01, -9.2551e-01, -3.3991e-01, -1.0394e+00, -6.7203e-02,\n",
            "        -2.1379e-01, -4.7690e-01,  2.1377e-01, -8.4008e-01,  5.2536e-02,\n",
            "         5.9298e-01,  2.9604e-01, -6.7644e-01,  1.3916e-01, -1.5504e+00,\n",
            "        -2.0765e-01,  7.2220e-01,  5.2056e-01, -7.6221e-02, -1.5194e-01,\n",
            "        -1.3134e-01,  5.8617e-02, -3.1869e-01, -6.1419e-01, -6.2393e-01,\n",
            "        -4.1548e-01, -3.8175e-02, -3.9804e-01,  4.7647e-01, -1.5983e-01])\n",
            "Token[4]: \"Italy\"\n",
            "tensor([ 0.4965, -0.1044,  0.4443,  0.5027, -0.0233,  0.1348, -0.2014,  0.3697,\n",
            "        -0.2545,  0.3423,  0.9152, -0.4945, -0.9644,  0.6179, -0.5277, -0.0154,\n",
            "         0.6764, -0.9327, -0.8452,  0.5719, -0.0767,  0.7369,  0.5728,  1.1469,\n",
            "        -0.1478, -1.1035,  0.2770,  0.1637,  0.4662, -0.7348, -0.6293, -0.1940,\n",
            "        -0.1950, -0.2131,  0.6181,  0.2442,  1.0047, -0.2529, -0.3938, -0.4351,\n",
            "        -0.7265, -0.5500,  1.2234, -0.5817,  0.5577, -0.2179,  0.9775, -0.1972,\n",
            "         0.6900,  0.0323,  0.4363,  0.9817, -0.5833,  1.0221, -0.8700, -2.4023,\n",
            "        -0.7888,  0.3990,  0.8996,  0.4941,  0.0194,  0.6253,  0.2678, -0.3875,\n",
            "         0.2334,  0.2968, -1.0096,  0.2509,  0.3046, -0.1964,  0.5537, -0.0314,\n",
            "        -0.3111,  0.4433,  0.8800, -0.2334, -1.2125, -0.0348, -0.0952,  0.0894,\n",
            "         0.7289, -0.5691, -0.6065,  0.4531, -0.2536, -0.9153, -0.9606,  0.9784,\n",
            "        -0.5361, -0.7378, -0.2457, -0.4487,  0.6726,  0.0828, -0.1711,  0.2618,\n",
            "        -0.5483,  0.0259,  0.6696, -0.2172])\n",
            "Token[5]: \"to\"\n",
            "tensor([-1.8970e-01,  5.0024e-02,  1.9084e-01, -4.9184e-02, -8.9737e-02,\n",
            "         2.1006e-01, -5.4952e-01,  9.8377e-02, -2.0135e-01,  3.4241e-01,\n",
            "        -9.2677e-02,  1.6100e-01, -1.3268e-01, -2.8160e-01,  1.8737e-01,\n",
            "        -4.2959e-01,  9.6039e-01,  1.3972e-01, -1.0781e+00,  4.0518e-01,\n",
            "         5.0539e-01, -5.5064e-01,  4.8440e-01,  3.8044e-01, -2.9055e-03,\n",
            "        -3.4942e-01, -9.9696e-02, -7.8368e-01,  1.0363e+00, -2.3140e-01,\n",
            "        -4.7121e-01,  5.7126e-01, -2.1454e-01,  3.5958e-01, -4.8319e-01,\n",
            "         1.0875e+00,  2.8524e-01,  1.2447e-01, -3.9248e-02, -7.6732e-02,\n",
            "        -7.6343e-01, -3.2409e-01, -5.7490e-01, -1.0893e+00, -4.1811e-01,\n",
            "         4.5120e-01,  1.2112e-01, -5.1367e-01, -1.3349e-01, -1.1378e+00,\n",
            "        -2.8768e-01,  1.6774e-01,  5.5804e-01,  1.5387e+00,  1.8859e-02,\n",
            "        -2.9721e+00, -2.4216e-01, -9.2495e-01,  2.1992e+00,  2.8234e-01,\n",
            "        -3.4780e-01,  5.1621e-01, -4.3387e-01,  3.6852e-01,  7.4573e-01,\n",
            "         7.2102e-02,  2.7931e-01,  9.2569e-01, -5.0336e-02, -8.5856e-01,\n",
            "        -1.3580e-01, -9.2551e-01, -3.3991e-01, -1.0394e+00, -6.7203e-02,\n",
            "        -2.1379e-01, -4.7690e-01,  2.1377e-01, -8.4008e-01,  5.2536e-02,\n",
            "         5.9298e-01,  2.9604e-01, -6.7644e-01,  1.3916e-01, -1.5504e+00,\n",
            "        -2.0765e-01,  7.2220e-01,  5.2056e-01, -7.6221e-02, -1.5194e-01,\n",
            "        -1.3134e-01,  5.8617e-02, -3.1869e-01, -6.1419e-01, -6.2393e-01,\n",
            "        -4.1548e-01, -3.8175e-02, -3.9804e-01,  4.7647e-01, -1.5983e-01])\n",
            "Token[6]: \"watch\"\n",
            "tensor([-0.3826, -0.0897,  0.0247, -0.7572, -0.4756,  0.6184, -0.4887,  0.6104,\n",
            "        -0.3182, -0.6345,  0.2959, -0.0163,  0.2665, -0.5765,  0.0370,  0.2765,\n",
            "         0.3202,  0.3467, -0.1426,  0.0792, -0.0280,  0.1436,  0.3244, -0.3482,\n",
            "         0.5716,  0.2056,  0.0053, -0.0093,  0.7061,  0.1924,  0.1740, -0.0148,\n",
            "         0.0962,  0.7506,  0.3824,  0.1172, -0.4454, -0.0183,  0.6754, -0.6029,\n",
            "        -0.3040,  0.0235, -0.3648, -0.3741, -0.3128, -0.0287, -0.1232, -0.2888,\n",
            "        -0.1828, -1.2734,  0.4381, -0.0581, -0.1152,  0.7380,  0.1000, -2.0191,\n",
            "        -0.1078,  0.2051,  1.5224,  0.1657, -0.4080,  1.1710, -0.0155, -0.6227,\n",
            "         0.2992,  0.5142, -0.0428,  0.6814,  0.1252,  0.4871,  0.2799, -0.4528,\n",
            "         0.3851, -0.1660,  0.0296,  0.1068,  0.1578,  0.1660,  0.1163, -0.1154,\n",
            "         0.7183, -0.0832, -0.1512,  0.3005, -1.1409, -0.5902, -0.5059, -0.0265,\n",
            "         0.3541, -0.1174,  0.6272, -0.2937,  0.4149, -0.9140, -0.3828,  0.0461,\n",
            "         0.0246,  0.1637,  0.4389,  0.2892])\n",
            "Token[7]: \"a\"\n",
            "tensor([-0.2709,  0.0440, -0.0203, -0.1740,  0.6444,  0.7121,  0.3551,  0.4714,\n",
            "        -0.2964,  0.5443, -0.7229, -0.0048,  0.0406,  0.0432,  0.2973,  0.1072,\n",
            "         0.4016, -0.5366,  0.0334,  0.0674,  0.6456, -0.0855,  0.1410,  0.0945,\n",
            "         0.7495, -0.1940, -0.6874, -0.4174, -0.2281,  0.1200, -0.4900,  0.8094,\n",
            "         0.0451, -0.1190,  0.2016,  0.3928, -0.2012,  0.3135,  0.7530,  0.2591,\n",
            "        -0.1157, -0.0293,  0.9350, -0.3607,  0.5242,  0.2371,  0.5271,  0.2287,\n",
            "        -0.5196, -0.7935, -0.2037, -0.5019,  0.1875,  0.9428, -0.4483, -3.6792,\n",
            "         0.0442, -0.2675,  2.1997,  0.2410, -0.0334,  0.6955, -0.6447, -0.0072,\n",
            "         0.8957,  0.2001,  0.4649,  0.6193, -0.1066,  0.0869, -0.4623,  0.1826,\n",
            "        -0.1585,  0.0208,  0.1937,  0.0634, -0.3167, -0.4818, -1.3848,  0.1367,\n",
            "         0.9686,  0.0500, -0.2738, -0.0357, -1.0577, -0.2447,  0.9037, -0.1244,\n",
            "         0.0808, -0.8340,  0.5720,  0.0889, -0.4253, -0.0183, -0.0800, -0.2858,\n",
            "        -0.0109, -0.4923,  0.6369,  0.2364])\n",
            "Token[8]: \"famous\"\n",
            "tensor([-0.0459,  0.0331, -0.1816,  0.2767,  0.4105,  0.4320, -0.2093, -0.4398,\n",
            "        -0.7052, -0.2525, -0.0284, -0.5213,  0.1193,  0.2284, -0.4385,  0.0422,\n",
            "         1.2009, -0.3105, -0.2019,  0.6664,  0.4182,  0.2137,  0.0295, -0.3447,\n",
            "         0.4922, -0.4525, -0.5364, -0.2629,  0.2013, -0.2590, -0.4128,  0.2062,\n",
            "         0.6130,  0.0720, -0.0648,  0.3338, -0.2617,  0.0839,  0.1069, -0.4902,\n",
            "         0.1112,  0.5452,  0.5832, -0.2009,  0.7128,  0.1952,  0.3961,  0.0212,\n",
            "         0.1609,  0.0681, -0.2690, -0.2029,  0.7475,  0.3446, -0.7366, -2.0202,\n",
            "        -0.8767,  0.4038,  0.6166, -0.4044, -0.3570,  1.6690,  0.4137,  0.1970,\n",
            "         0.8426, -0.4516,  0.6435,  0.1850,  0.1836,  0.0199, -0.3201,  0.4093,\n",
            "         0.1664,  0.1906, -0.1364, -0.2711,  0.3763, -0.0647, -0.1257, -0.0635,\n",
            "         0.5196, -0.2004,  0.2479, -0.1565, -1.3767,  0.0092, -0.8601, -0.6435,\n",
            "         0.2971, -0.4727,  0.4316, -0.1106,  0.3009, -0.0596, -0.8374, -0.1543,\n",
            "        -1.3226, -0.8149,  0.3884,  0.2985])\n",
            "Token[9]: \"play\"\n",
            "tensor([-0.2408,  0.0247,  0.6461, -0.4000, -0.3512,  0.7456,  0.2530,  0.1407,\n",
            "        -0.9319, -0.3551, -0.0583, -0.4629, -0.3528,  0.1506, -0.1548,  0.2209,\n",
            "         0.1969,  0.9385, -0.3012,  0.6651,  0.0238,  0.1202,  0.4089,  0.3576,\n",
            "         0.7272, -0.3942, -0.3571, -0.5079,  0.7247,  0.5239, -1.4761,  0.9837,\n",
            "         0.1517, -0.2047,  0.4378, -0.3446, -0.5340,  0.5334, -0.6866, -0.5667,\n",
            "         0.3157, -0.0532, -0.1194, -0.1369, -0.1898, -0.1227,  0.1451, -0.6482,\n",
            "         0.2514, -1.2370, -0.6425,  0.4000, -0.0588,  0.7735,  0.2392, -2.9341,\n",
            "        -0.3087, -0.4429,  0.6963,  0.9167, -0.6856,  0.9386, -0.7600, -0.1033,\n",
            "         0.5508, -0.0460,  0.2931,  0.6355, -0.6446, -0.0816, -0.0425, -0.6625,\n",
            "         0.5626, -0.4048,  0.2786, -0.1148, -0.4131, -0.0099,  0.1606,  0.1285,\n",
            "         0.4992, -0.0717, -0.5237, -0.0472, -1.7793, -0.1810, -0.3947,  0.1824,\n",
            "        -0.1078, -0.2051, -0.5151,  0.1035, -0.3436,  0.1991, -0.4173,  0.0461,\n",
            "        -0.1958,  0.0104,  0.4489,  0.5344])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2lTTOZ8y-PL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d1325ec-cd18-481d-e168-ede5868e373a"
      },
      "source": [
        "#print the embedding for the word \"play\"\n",
        "\n",
        "print(colored(\"The embedding of the word play\",attrs=['bold']))\n",
        "print(glove_sentence[9].embedding)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The embedding of the word play\n",
            "tensor([-0.2408,  0.0247,  0.6461, -0.4000, -0.3512,  0.7456,  0.2530,  0.1407,\n",
            "        -0.9319, -0.3551, -0.0583, -0.4629, -0.3528,  0.1506, -0.1548,  0.2209,\n",
            "         0.1969,  0.9385, -0.3012,  0.6651,  0.0238,  0.1202,  0.4089,  0.3576,\n",
            "         0.7272, -0.3942, -0.3571, -0.5079,  0.7247,  0.5239, -1.4761,  0.9837,\n",
            "         0.1517, -0.2047,  0.4378, -0.3446, -0.5340,  0.5334, -0.6866, -0.5667,\n",
            "         0.3157, -0.0532, -0.1194, -0.1369, -0.1898, -0.1227,  0.1451, -0.6482,\n",
            "         0.2514, -1.2370, -0.6425,  0.4000, -0.0588,  0.7735,  0.2392, -2.9341,\n",
            "        -0.3087, -0.4429,  0.6963,  0.9167, -0.6856,  0.9386, -0.7600, -0.1033,\n",
            "         0.5508, -0.0460,  0.2931,  0.6355, -0.6446, -0.0816, -0.0425, -0.6625,\n",
            "         0.5626, -0.4048,  0.2786, -0.1148, -0.4131, -0.0099,  0.1606,  0.1285,\n",
            "         0.4992, -0.0717, -0.5237, -0.0472, -1.7793, -0.1810, -0.3947,  0.1824,\n",
            "        -0.1078, -0.2051, -0.5151,  0.1035, -0.3436,  0.1991, -0.4173,  0.0461,\n",
            "        -0.1958,  0.0104,  0.4489,  0.5344])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQDHTNOB7Zir",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c2dad10-f9a8-4c67-fbf8-03cf72ce1f88"
      },
      "source": [
        "#print the length of the embedding vector\n",
        "print(colored(\"The size of the embedding vector of the word play\",attrs=['bold']))\n",
        "len(glove_sentence[9].embedding)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The size of the embedding vector of the word play\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jad27v-Pqvvq"
      },
      "source": [
        "Let's create another sentence that contains the word **\"play\"** but with a different meaning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXbFZZLgyxUz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2571a1e-1355-4ade-8189-af32ea6a2fc4"
      },
      "source": [
        "# create sentence.\n",
        "glove_sentence2 = Sentence('They play tennis on their break')\n",
        "\n",
        "# embed a sentence using glove.\n",
        "glove_embedding.embed(glove_sentence2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Sentence[6]: \"They play tennis on their break\"]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZB-zK48zMiu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b76edeb2-aa97-4ff2-ce31-18329f16b145"
      },
      "source": [
        "#print the embedding of the word \"play\" in the first sentence\n",
        "print(colored(\"The embedding of the word play in the first sentence\",attrs=['bold']))\n",
        "print(glove_sentence[9].embedding)\n",
        "\n",
        "#print the embedding for the word \"play\" you will notice it is similar to the emebdding of \"play\" in the previous sentence\n",
        "print(colored(\"The embedding of the word play in the second sentence\",attrs=['bold']))\n",
        "print(glove_sentence2[1].embedding)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The embedding of the word play in the first sentence\n",
            "tensor([-0.2408,  0.0247,  0.6461, -0.4000, -0.3512,  0.7456,  0.2530,  0.1407,\n",
            "        -0.9319, -0.3551, -0.0583, -0.4629, -0.3528,  0.1506, -0.1548,  0.2209,\n",
            "         0.1969,  0.9385, -0.3012,  0.6651,  0.0238,  0.1202,  0.4089,  0.3576,\n",
            "         0.7272, -0.3942, -0.3571, -0.5079,  0.7247,  0.5239, -1.4761,  0.9837,\n",
            "         0.1517, -0.2047,  0.4378, -0.3446, -0.5340,  0.5334, -0.6866, -0.5667,\n",
            "         0.3157, -0.0532, -0.1194, -0.1369, -0.1898, -0.1227,  0.1451, -0.6482,\n",
            "         0.2514, -1.2370, -0.6425,  0.4000, -0.0588,  0.7735,  0.2392, -2.9341,\n",
            "        -0.3087, -0.4429,  0.6963,  0.9167, -0.6856,  0.9386, -0.7600, -0.1033,\n",
            "         0.5508, -0.0460,  0.2931,  0.6355, -0.6446, -0.0816, -0.0425, -0.6625,\n",
            "         0.5626, -0.4048,  0.2786, -0.1148, -0.4131, -0.0099,  0.1606,  0.1285,\n",
            "         0.4992, -0.0717, -0.5237, -0.0472, -1.7793, -0.1810, -0.3947,  0.1824,\n",
            "        -0.1078, -0.2051, -0.5151,  0.1035, -0.3436,  0.1991, -0.4173,  0.0461,\n",
            "        -0.1958,  0.0104,  0.4489,  0.5344])\n",
            "The embedding of the word play in the second sentence\n",
            "tensor([-0.2408,  0.0247,  0.6461, -0.4000, -0.3512,  0.7456,  0.2530,  0.1407,\n",
            "        -0.9319, -0.3551, -0.0583, -0.4629, -0.3528,  0.1506, -0.1548,  0.2209,\n",
            "         0.1969,  0.9385, -0.3012,  0.6651,  0.0238,  0.1202,  0.4089,  0.3576,\n",
            "         0.7272, -0.3942, -0.3571, -0.5079,  0.7247,  0.5239, -1.4761,  0.9837,\n",
            "         0.1517, -0.2047,  0.4378, -0.3446, -0.5340,  0.5334, -0.6866, -0.5667,\n",
            "         0.3157, -0.0532, -0.1194, -0.1369, -0.1898, -0.1227,  0.1451, -0.6482,\n",
            "         0.2514, -1.2370, -0.6425,  0.4000, -0.0588,  0.7735,  0.2392, -2.9341,\n",
            "        -0.3087, -0.4429,  0.6963,  0.9167, -0.6856,  0.9386, -0.7600, -0.1033,\n",
            "         0.5508, -0.0460,  0.2931,  0.6355, -0.6446, -0.0816, -0.0425, -0.6625,\n",
            "         0.5626, -0.4048,  0.2786, -0.1148, -0.4131, -0.0099,  0.1606,  0.1285,\n",
            "         0.4992, -0.0717, -0.5237, -0.0472, -1.7793, -0.1810, -0.3947,  0.1824,\n",
            "        -0.1078, -0.2051, -0.5151,  0.1035, -0.3436,  0.1991, -0.4173,  0.0461,\n",
            "        -0.1958,  0.0104,  0.4489,  0.5344])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWxCSU3M179t"
      },
      "source": [
        "Check if the word **\"play\"** have the same embeddings in both sentences when **GloVe** was used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrQvADTU1fxh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73c5456b-2897-4300-9b4d-3885f293b9fd"
      },
      "source": [
        "from scipy import spatial\n",
        "similarity= 1 - spatial.distance.cosine(glove_sentence[9].embedding, glove_sentence2[1].embedding)\n",
        "similarity"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uozu0MFjUoB4"
      },
      "source": [
        "**Notice that the similarity between the words is equal 1 when GloVe was used which means they are exacly similar.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmvFsKKN9WeX"
      },
      "source": [
        "### **Exercise1**\n",
        "Choose either the word \"rose\" or \"tie\" to create two different sentences such that they share the same word but with different meanings. Use GloVe to get the words embeddings. Check the similarity between the embeddings of the common word in both sentences when GloVe was used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01fOLbFa9muy"
      },
      "source": [
        "#add your solution here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QBkFAYF50BY"
      },
      "source": [
        "### **References**\n",
        "\n",
        "* https://radimrehurek.com/gensim/index.html\n",
        "*   https://radimrehurek.com/gensim/models/word2vec.html\n",
        "*    Word Embeddings introduction: https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/\n",
        "* Word2Vec introduction: http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\n",
        "* A great Gensim implentation tutorial: http://kavita-ganesan.com/gensim-word2vec-tutorial-starter-code/#.W467ScBjM2x\n",
        "* Original articles from Mikolov et al.: https://arxiv.org/abs/1301.3781 and https://arxiv.org/abs/1310.4546\n",
        "\n",
        "*   [Flair word emebddings tutorial.](https://github.com/flairNLP/flair/blob/master/resources/docs/TUTORIAL_3_WORD_EMBEDDING.md)\n",
        "*   [Flair Elmo embedding tutorial.](https://github.com/flairNLP/flair/blob/master/resources/docs/embeddings/ELMO_EMBEDDINGS.**md**)\n",
        "* [Flair document embeddings tutorial.](https://github.com/flairNLP/flair/blob/master/resources/docs/TUTORIAL_5_DOCUMENT_EMBEDDINGS.md)\n",
        "\n"
      ]
    }
  ]
}